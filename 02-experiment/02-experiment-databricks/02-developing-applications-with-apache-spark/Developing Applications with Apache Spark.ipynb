{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b53bd3a-c15a-422d-96fe-177e28ce0fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Developing Applications with Apache Spark\n",
    "\n",
    "Master scalable data processing with Apache Spark in this hands-on course. Learn to build efficient ETL pipelines, perform advanced analytics, and optimize distributed transformations using Sparkâ€™s DataFrame API. Explore grouping, aggregation, joins, set operations, and window functions. You'll also work with complex data types like arrays, maps, and structs, applying best practices for performance tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "You should meet the following prerequisites before starting this course:\n",
    "\n",
    "- Basic programming knowledge\n",
    "- Familiarity with Python\n",
    "- Understanding of basic SQL (`SELECT`, `JOIN`, `GROUP BY`)\n",
    "- Knowledge of data processing concepts\n",
    "- Completion of **Introduction to Apache Spark** or prior Databricks experience\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84698cbd-962d-45b9-bf65-eb2cea159c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Grouping and Aggregating Data\n",
    "\n",
    "This demonstration will show how to perform grouping and aggregation operations using NYC Taxi trip data. We'll explore basic grouping, multiple aggregations, and window functions.\n",
    "\n",
    "### Objectives\n",
    "- Understand basic grouping operations in Spark\n",
    "- Perform time-based analysis using aggregations\n",
    "- Implement complex aggregations with multiple metrics\n",
    "- Use window functions for advanced analytics\n",
    "- Optimize aggregation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2337e607-3282-44ef-9bc7-1b4a63acfbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Setup and Loading\n",
    "\n",
    "First, let's load our taxi trip data and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1768e4-31d6-4c3e-a4b5-4a93afc27edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read and displaying the taxi data\n",
    "trips_df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "\n",
    "display(trips_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce75dce-9fe2-4141-8c44-2b396f7382fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Grouping Operations\n",
    "\n",
    "Let's start with simple grouping operations to understand trip patterns by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01723e1-d4bc-48b2-9666-648cee28198e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count trips by pickup location, to show top 5 most popular pickup locations\n",
    "location_counts = trips_df \\\n",
    "    .groupBy(\"pickup_zip\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "display(location_counts.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a88da0-1d8c-4df6-b6da-6703f19be176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combining Multiple Aggregations\n",
    "\n",
    "Let's perform multiple aggregations by location using the `agg()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba121d-40ba-4cd4-87c8-21e6868f789d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform multiple aggregations by location, order by most popular pickup locations\n",
    "location_stats = trips_df \\\n",
    "    .groupBy(\"pickup_zip\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_trips\"),\n",
    "        round(avg(\"trip_distance\"), 2).alias(\"avg_distance\"),\n",
    "        round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "        round(sum(\"fare_amount\"), 2).alias(\"total_fare_amt\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_trips\"))\n",
    "\n",
    "display(location_stats.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6833af08-8963-4644-96bf-8afe7b542568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Window Functions\n",
    "\n",
    "Now let's use window functions for more advanced analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0fe8fe-81c7-479e-9cd8-2079119d6439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create window specs for different ranking methods\n",
    "window_by_trips = Window.orderBy(desc(\"total_trips\"))\n",
    "window_by_fare = Window.orderBy(desc(\"avg_fare\"))\n",
    "\n",
    "# Add different types of rankings\n",
    "ranked_locations = location_stats \\\n",
    "    .withColumn(\"trips_rank\", rank().over(window_by_trips)) \\\n",
    "    .withColumn(\"fare_rank\", rank().over(window_by_fare)) \\\n",
    "    .withColumn(\"fare_quintile\", ntile(5).over(window_by_fare))  # Divide into 5 groups by fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfb0df7-8478-48b8-8aa0-9ad21508fb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ranked_locations.createOrReplaceTempView(\"ranked_locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8ccc21-2937-4a3c-9494-2f0e088fbabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select fare_quintile,min(avg_fare),max(avg_fare),count(*) as cnt_per_group from ranked_locations group by fare_quintile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56d84b3-7b27-42fa-bb04-a61ad7fa124c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the results\n",
    "display(ranked_locations.select(\n",
    "    \"pickup_zip\", \n",
    "    \"total_trips\", \n",
    "    \"avg_fare\", \n",
    "    \"avg_distance\",\n",
    "    \"trips_rank\",\n",
    "    \"fare_rank\",\n",
    "    \"fare_quintile\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531f2b01-2ef0-42d6-937d-603e0ed2f1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Basic Grouping**\n",
    "   - Use `groupBy()` followed by aggregation method\n",
    "   - Can group by multiple columns\n",
    "   - Always check data distribution\n",
    "\n",
    "2. **Window Functions**\n",
    "   - Perfect for comparative analytics\n",
    "   - Consider performance impact\n",
    "   - Use appropriate window frame\n",
    "\n",
    "3. **Best Practices**\n",
    "   - Always alias aggregated columns\n",
    "   - Handle null values appropriately\n",
    "   - Consider data skew in grouping keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bd771cf-eaeb-4255-a74f-e5dedf5ec3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Grouping and Aggregating E-Commerce Data\n",
    "\n",
    "In this lab, you'll practice working with grouping and aggregation in Spark using a dataset of e-commerce transactions. You'll perform various analyses to uncover patterns and insights in customer purchasing behavior.\n",
    "\n",
    "### Objectives\n",
    "- Use `groupBy` operations to summarize data\n",
    "- Implement multiple aggregations\n",
    "- Apply different ordering techniques\n",
    "- (Bonus) Use window functions for advanced analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6070976-edca-47de-b07f-e8fce073af32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Load the retail transactions data and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5a283f-b816-44dc-b3a2-5a034c58e77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "## Read the e-commerce transactions data\n",
    "transactions_df = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "\n",
    "## display a sample of the data\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eafb537c-ef7c-40db-b6c5-9f31865e6d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Grouping Operations\n",
    "\n",
    "Let's start with simple grouping operations to understand product sales patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19e359ca-9e71-4b2a-b485-b895d9a0dd9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Group the data by products and count the number of sales\n",
    "# 2. Order the results by the most popular products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6fbc061-fe80-4b00-9778-8820a6ae206d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DataFrame Relational Operations in Spark\n",
    "\n",
    "This demonstration shows how to effectively use joins and set operations with DataFrames, focusing on performance optimization and best practices.\n",
    "\n",
    "### Objectives\n",
    "- Understand different types of DataFrame joins\n",
    "- Implement performance optimizations for joins\n",
    "- Handle complex join scenarios\n",
    "- Use set operations effectively\n",
    "- Apply best practices for data skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6179c291-db1a-4aad-830e-88241c3d87e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "First, let's load our sample retail data tables and examine their structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8559195-deaf-485f-8132-07b0b154560a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read the data \n",
    "transactions_df = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "customers_df = spark.read.table(\"samples.bakehouse.sales_customers\")\n",
    "franchises_df = spark.read.table(\"samples.bakehouse.sales_franchises\")\n",
    "suppliers_df = spark.read.table(\"samples.bakehouse.sales_suppliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8be658ef-9fef-4476-9c91-2971b092b191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examine schemas\n",
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb922c17-fc60-48be-8a8e-8fd04903c6fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b4df76c-2f1a-4f7d-8979-ea52d0454edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "franchises_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f2198c0-1947-48f0-87f1-4beb50fdc64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "suppliers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c638d9-68b5-4275-b2ed-a8120608a362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Join Operations\n",
    "\n",
    "Let's start with simple join operations to combine our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c76e2f1-2e18-44c2-9856-2338e7efb52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner join example to enrich the transactions with store information\n",
    "enriched_transactions = franchises_df.join(\n",
    "    transactions_df,\n",
    "    on=\"franchiseID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "display(enriched_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "961b9bf8-546a-4b86-9345-09608b5fe072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The \"on\" clause can contain an expression\n",
    "enriched_transactions = franchises_df.join(\n",
    "    transactions_df,\n",
    "    on= transactions_df.franchiseID == franchises_df.franchiseID,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "display(enriched_transactions)\n",
    "\n",
    "# This is particularly useful if the join key is named differently in both entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5b8c550-bc5c-4ec8-b6f9-5e8157003cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Please note how all fields from both dataframes are present in the result, a better practice is to project the columns you need from each entity\n",
    "# We will also alias some of the columns to disambiguate column names\n",
    "enriched_transactions = franchises_df \\\n",
    "    .select(\n",
    "        \"franchiseID\", \n",
    "        col(\"name\").alias(\"store_name\"), \n",
    "        col(\"city\").alias(\"store_city\"), \n",
    "        col(\"country\").alias(\"store_country\")\n",
    "        ) \\\n",
    "    .join(\n",
    "        transactions_df,\n",
    "        on=\"franchiseID\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "display(enriched_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b324e4bc-bbed-495e-a351-bd4a5f690f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Full Outer Join Operations\n",
    "\n",
    "Let's analyze the relationships between dataframes and identify missing data using outer joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f51e6cae-ae67-460d-ac59-85a48707a4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's analyze the relationship between franchises and suppliers using a full outer join\n",
    "full_join = franchises_df \\\n",
    "    .withColumnRenamed(\"name\", \"franchise_name\") \\\n",
    "    .join(\n",
    "        suppliers_df.select(\"supplierID\", col(\"name\").alias(\"supplier_name\")),\n",
    "        on=\"supplierID\",\n",
    "        how=\"full_outer\" # Doing outer join\n",
    "    )\n",
    "\n",
    "# Find records that would NOT appear in an inner join\n",
    "# These are records where either franchises or suppliers data is null\n",
    "non_matching_records = full_join.filter(\n",
    "        col(\"franchiseID\").isNull() | \n",
    "        col(\"supplier_name\").isNull()\n",
    "    ) \\\n",
    "    .select(\"franchiseID\", \"franchise_name\", col(\"supplierID\").alias(\"orphaned_supplier_id\"))\n",
    "\n",
    "display(non_matching_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5f69c72-3dd4-45d5-bcf6-62f2b1a4d96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using Spark SQL\n",
    "\n",
    "Let's do this using Spark SQL now....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224f7591-1278-49e1-84f3-71d37fac0d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary views\n",
    "franchises_df.createOrReplaceTempView(\"franchises\")\n",
    "suppliers_df.createOrReplaceTempView(\"suppliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbed5c23-cb18-47be-897e-c5db9bf2d1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Let's do our outer join using SQL\n",
    "SELECT \n",
    "    f.franchiseID,\n",
    "    f.name as franchise_name,\n",
    "    f.supplierID as orphaned_supplier_id\n",
    "FROM franchises f\n",
    "FULL OUTER JOIN suppliers s\n",
    "ON f.supplierID = s.supplierID\n",
    "WHERE f.franchiseID IS NULL OR s.name IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f63a834c-7fad-4a56-8444-8cf6002c8ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set Operations\n",
    "\n",
    "Now let's explore relationships using set operations using the DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb18aa9a-53cd-44d5-843f-a9a62d01a320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify supplier IDs in each DataFrame\n",
    "franchise_suppliers = franchises_df.select(\"supplierID\").distinct()\n",
    "all_suppliers = suppliers_df.select(\"supplierID\").distinct()\n",
    "\n",
    "# Find supplierIDs that are in franchises_df but not in suppliers_df\n",
    "franchises_without_valid_suppliers = franchise_suppliers.subtract(all_suppliers)\n",
    "display(franchises_without_valid_suppliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb4c8b2-2d70-47ff-89cb-e24269237c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the overlap - suppliers that exist in both tables\n",
    "common_suppliers = franchise_suppliers.intersect(all_suppliers)\n",
    "display(common_suppliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d37f500a-32ce-4815-9bd6-bf521ee0e9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Join Strategy**\n",
    "   - Use inner joins where keys exist in all dataframes\n",
    "   - Use outer joins where there is a possibility that keys don't exist in both dataframes\n",
    "   - Handle column name conflicts\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Filter before joining\n",
    "   - Project only needed columns\n",
    "   - Handle skewed keys appropriately\n",
    "   - Reference the smaller dataframe first; or\n",
    "   - Use broadcast joins for small tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19214e1f-da67-4270-8338-c4f8bb06dcc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Group the data by products and count the number of sales\n",
    "product_counts = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86067a5c-848f-47a6-8b76-7a5d240b4419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multiple Aggregations\n",
    "\n",
    "Now let's perform multiple aggregations to get deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f962c80d-c879-4578-9c49-54948d4cf7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Analyze sales by payment method\n",
    "# 2. Calculate the total revenue, average transaction value, and count of transactions for each payment method\n",
    "# 3. Order by total revenue (highest first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a317a2af-2871-4a33-a91f-02bd3956081a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Analyze sales by payment method\n",
    "payment_analysis = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "427b5d02-801e-462d-9a77-5a6dbb617309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bonus Challenge: Window Functions\n",
    "\n",
    "If you have time, try using window functions for advanced analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1205ed46-3eda-4bd7-9cc8-e14e3bafeedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## First, calculate total revenue by product\n",
    "product_revenue_df = transactions_df \\\n",
    "    .groupBy(\"product\") \\\n",
    "    .agg(\n",
    "        round(sum(col(\"totalPrice\")), 2).alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "## Use window functions to add rankings\n",
    "## Rank products by total revenue\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa31991a-cbb3-4763-ba72-3291d0eba83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Working with Complex Data Types in Spark\n",
    "\n",
    "This demonstration shows how to effectively work with nested data structures in Spark, including structs, arrays, and maps, using real e-commerce data examples.\n",
    "\n",
    "### Objectives\n",
    "- Convert JSON string data to Spark SQL native complex types\n",
    "- Understand and manipulate complex data types (Struct, Array, Map)\n",
    "- Process nested JSON-like data structures\n",
    "- Use the pivot and explode functions to reshape datasets as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9420d9-1da9-4bf8-bf6d-0da106652821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0451aac-f3bb-440a-98c0-3fce0c0a0da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "# Define the CSV-like data with JSON strings as a list of tuples\n",
    "data = [\n",
    "    (101, \"Alice Smith\", \"true\", \"2023-01-15\",\n",
    "     \"\"\"[\"hiking\", \"machine learning\", \"photography\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P123\", \"name\": \"Laptop\", \"price\": 1299.99, \"date\": \"2023-03-10\"}, {\"product_id\": \"P456\", \"name\": \"External Monitor\", \"price\": 249.99, \"date\": \"2023-03-15\"}]\"\"\"),\n",
    "    \n",
    "    (102, \"Bob Johnson\", \"true\", \"2023-02-20\",\n",
    "     \"\"\"[\"coding\", \"gaming\", \"reading\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P789\", \"name\": \"Mechanical Keyboard\", \"price\": 149.99, \"date\": \"2023-04-05\"}]\"\"\"),\n",
    "    \n",
    "    (103, \"Charlie Williams\", \"false\", \"2022-11-05\",\n",
    "     \"\"\"[\"golf\", \"cooking\", \"traveling\"]\"\"\",\n",
    "     \"\"\"[]\"\"\"),\n",
    "    \n",
    "    (104, \"Diana Garcia\", \"true\", \"2023-03-10\",\n",
    "     \"\"\"[\"drawing\", \"yoga\", \"music\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P234\", \"name\": \"Graphics Tablet\", \"price\": 199.99, \"date\": \"2023-03-25\"}, {\"product_id\": \"P567\", \"name\": \"Stylus Pen\", \"price\": 49.99, \"date\": \"2023-03-25\"}, {\"product_id\": \"P890\", \"name\": \"Design Software\", \"price\": 299.99, \"date\": \"2023-04-02\"}]\"\"\"),\n",
    "    \n",
    "    (105, \"Ethan Davis\", \"true\", \"2022-09-15\",\n",
    "     \"\"\"[\"basketball\", \"programming\", \"movies\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P321\", \"name\": \"Textbook\", \"price\": 89.99, \"date\": \"2023-01-10\"}, {\"product_id\": \"P654\", \"name\": \"Backpack\", \"price\": 59.99, \"date\": \"2023-01-10\"}]\"\"\"),\n",
    "    \n",
    "    (106, \"Fiona Miller\", \"true\", \"2023-04-01\",\n",
    "     \"\"\"[\"social media\", \"writing\", \"photography\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P987\", \"name\": \"Camera\", \"price\": 599.99, \"date\": \"2023-04-15\"}]\"\"\"),\n",
    "    \n",
    "    (107, \"George Wilson\", \"false\", \"2022-12-10\",\n",
    "     \"\"\"[\"finance\", \"cycling\", \"chess\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P111\", \"name\": \"Financial Software\", \"price\": 199.99, \"date\": \"2023-01-05\"}, {\"product_id\": \"P222\", \"name\": \"Wireless Mouse\", \"price\": 29.99, \"date\": \"2023-02-15\"}]\"\"\"),\n",
    "    \n",
    "    (108, \"Hannah Brown\", \"true\", \"2023-02-28\",\n",
    "     \"\"\"[\"education\", \"reading\", \"gardening\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P333\", \"name\": \"Educational Subscription\", \"price\": 14.99, \"date\": \"2023-03-01\"}, {\"product_id\": \"P444\", \"name\": \"Notebook Set\", \"price\": 24.99, \"date\": \"2023-03-01\"}]\"\"\"),\n",
    "    \n",
    "    (109, \"Ian Taylor\", \"true\", \"2022-10-20\",\n",
    "     \"\"\"[\"cooking\", \"food\", \"travel\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P555\", \"name\": \"Cooking Knives\", \"price\": 179.99, \"date\": \"2023-01-25\"}, {\"product_id\": \"P666\", \"name\": \"Recipe Book\", \"price\": 39.99, \"date\": \"2023-02-10\"}, {\"product_id\": \"P777\", \"name\": \"Spice Set\", \"price\": 49.99, \"date\": \"2023-03-20\"}]\"\"\"),\n",
    "    \n",
    "    (110, \"Julia Martinez\", \"true\", \"2023-03-15\",\n",
    "     \"\"\"[\"law\", \"politics\", \"hiking\"]\"\"\",\n",
    "     \"\"\"[{\"product_id\": \"P888\", \"name\": \"Legal Reference Book\", \"price\": 129.99, \"date\": \"2023-04-10\"}]\"\"\")\n",
    "]\n",
    "\n",
    "# Define the schema for the raw data\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"active\", StringType(), True),\n",
    "    StructField(\"signup_date\", StringType(), True),\n",
    "    StructField(\"interests\", StringType(), True),\n",
    "    StructField(\"recent_purchases\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame directly\n",
    "df_raw = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create Temp View\n",
    "df_raw.createOrReplaceTempView(\"raw_user_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d375911a-81a1-4c88-9473-2be1b9a18462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Querying the newly created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7039b2cf-57bc-4dc1-9e5c-32c5bccf9353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from raw_user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56e7b21-522a-4fff-8332-af9772a4afe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Convert from JSON Strings to StructTypes\n",
    "\n",
    "Given raw data which includes nested JSON strings (arrays and/or objects), we will convert this data to native `StructTypes` in the DataFrame API.\n",
    "\n",
    "#### Why Convert JSON Strings to StructTypes?\n",
    "\n",
    "JSON strings in Spark DataFrames come with several inefficiencies:\n",
    "\n",
    "1. **Parsing Overhead**: Every time you query JSON strings, Spark needs to parse them, adding computational overhead\n",
    "2. **Memory Inefficiency**: JSON strings store field names repeatedly for every row, wasting memory\n",
    "3. **No Type Safety**: JSON strings don't enforce data types, leading to potential errors\n",
    "4. **Poor Query Performance**: Spark can't optimize queries on JSON string content as effectively\n",
    "5. **Limited Predicate Pushdown**: Filter operations can't leverage columnar storage optimizations\n",
    "\n",
    "### Steps to Convert JSON Strings to StructTypes\n",
    "\n",
    "1. **Infer Schema**: Determine the structure of the JSON data (the `schema_of_json` function can be used for this)\n",
    "2. **Apply Schema**: Use `from_json()` to convert strings to structured data\n",
    "3. **Validate**: Ensure all data is correctly parsed and types are appropriate\n",
    "4. **Optimize**: Once converted, optimize storage/processing if needed\n",
    "\n",
    "### Benefits of StructTypes\n",
    "\n",
    "1. **Columnar Storage**: Efficient storage with Parquet/Delta\n",
    "2. **Type Safety**: Schema enforcement prevents data errors\n",
    "3. **Query Optimization**: Spark can optimize queries better with typed data\n",
    "4. **Predicate Pushdown**: Filters can be pushed down to storage layer\n",
    "5. **Better Performance**: Faster queries and reduced memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd9fb62-8c76-4a7e-8fa4-64c70771e68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load some data which includes JSON strings\n",
    "raw_user_data_df = spark.read.table(\"raw_user_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b52e5a8-d976-47e7-8aea-de7b8354e760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the Data\n",
    "raw_user_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0a2f82-d867-43a9-9aa8-0ace7a36b449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the Dataframe \n",
    "display(raw_user_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6889b4-cbb1-4120-a968-6fe2d071cd92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the data above and find columns **interests** and **recent_purchases**\n",
    "2. **interests** is an array column of String elements and **recent_purchases** is column containing objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb45764-a064-40b1-928e-5fe4f94d0b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Interests is an array of strings, using predefined schema\n",
    "interests_schema = ArrayType(StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a773e87-a038-4980-b5e2-cb833567d062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the schema for the \"recent_purchases\" and cast all of the columns from the raw data set into a confirmed structure\n",
    "\n",
    "# Take a sample of one value of the \"recent_purchases\" column, bring this back to the Driver\n",
    "recent_purchases_json = raw_user_data_df.select(\"recent_purchases\").limit(1).collect()[0][0]\n",
    "print(\"Raw JSON string:\", recent_purchases_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee6d03d-1048-4351-984a-f328cc7b7783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Use the **schema_of_json** Function to generate a Schema based upon a sample row of data\n",
    "\n",
    "In many cases, especially with multiple nested complex structures, it is easiest to generate a schema based upon a sample of JSON data, we can do this using the schema_of_json Function.  \n",
    "\n",
    "From the above dataset, we can see that **interests** is an array column of string elements, **recent_purchases** is an array column which contains objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789dfd4c-904c-4aa3-81d6-e4817a40a55f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the schema for the recent_purchases JSON\n",
    "recent_purchases_schema = schema_of_json(lit(recent_purchases_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87f7dc2-c737-48e7-925c-e6886f9034bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse columns with the correct schemas\n",
    "parsed_users_df = raw_user_data_df.select(\n",
    "    col(\"user_id\").cast(\"integer\"),\n",
    "    col(\"name\"),\n",
    "    col(\"active\").cast(\"boolean\"),\n",
    "    col(\"signup_date\").cast(\"date\"),\n",
    "    from_json(col(\"interests\"), interests_schema).alias(\"interests\"),\n",
    "    from_json(col(\"recent_purchases\"), recent_purchases_schema).alias(\"recent_purchases\")\n",
    ")\n",
    "\n",
    "# Examine the schema\n",
    "parsed_users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71545324-0ccf-4e68-aa48-6177e092bd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now look at the data again, You will notice order has been changed for recent_purchases\n",
    "display(parsed_users_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d6700a-0d50-4565-b1e1-e05e4cf713f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with Arrays\n",
    "\n",
    "Let's explore different ways to access and manipulate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c8fb52-946d-4537-902c-5713138fac34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the array_size method to see the lengths of the array columns in the dataframe\n",
    "display(\n",
    "parsed_users_df.select(\n",
    "    \"user_id\",\n",
    "    array_size(\"interests\").alias(\"number_of_interests\"),\n",
    "    array_size(\"recent_purchases\").alias(\"number_of_recent_purchases\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2100d2-4845-4ed8-be71-2ac52bb8cf43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The explode Method\n",
    "\n",
    "The `explode` method is used to unnest array elements into records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f37639-061c-4c04-90fa-d2106a1fa303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's start by simplifying the data by selecting only the columns we need\n",
    "user_101s_interests_df = parsed_users_df.select(\"user_id\", \"interests\").filter(parsed_users_df.user_id == 101)\n",
    "display(user_101s_interests_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f948d39f-198a-445f-ac99-7b43adfa7eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's demonstrate explode, note how there are three rows associated with \"user_id\" 101 (one for each \"interests\" array element)\n",
    "display(\n",
    "    user_101s_interests_df.select(\n",
    "        \"user_id\", \n",
    "        explode(\"interests\").alias(\"interest\")\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75c089d2-fda7-4bcf-9151-9b8278cacf9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The collect_set and collect_list Methods\n",
    "\n",
    "The `collect_set` and `collect_list` methods are aggregate functions (which typically operate on grouped data) to create arrays from column values.  \n",
    "\n",
    "`collect_list` may include duplicate values, while `collect_set` removes duplicate array elements should they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb7cabb-018b-4667-841a-b10c9cceec0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's start by creating a new DataFrame with the \"interests\" column exploded\n",
    "exploded_df = parsed_users_df.select(\"user_id\", explode(\"interests\").alias(\"interest\"))\n",
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "308e1c66-671e-4902-9921-f40c2587609b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's use `collect_list` to collect all the \"interests\" values into a list for each \"user_id\"\n",
    "user_interests_df = exploded_df.groupBy(\"user_id\").agg(collect_list(\"interest\").alias(\"interests\"))\n",
    "display(user_interests_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce132d2-3144-4eb1-a78e-51579de54677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Referencing Struct Fields\n",
    "\n",
    "Let's explore how to access fields within a struct (an object with a predefined schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbdfb196-7dc0-4018-8205-d0f2dd4e16ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First let's explode the \"recent_purchases\" column\n",
    "exploded_purchases_df = parsed_users_df.select(\"user_id\", explode(\"recent_purchases\").alias(\"purchase\"))\n",
    "display(exploded_purchases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9c5854-c6e3-4eef-b175-59f47c13c573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the dot notation to access struct fields\n",
    "recent_purchases_df = exploded_purchases_df.select(\n",
    "                        \"user_id\", \n",
    "                        col(\"purchase.date\").alias(\"purchase_date\"), \n",
    "                        col(\"purchase.product_id\").alias(\"product_id\"), \n",
    "                        col(\"purchase.name\").alias(\"product_name\"), \n",
    "                        col(\"purchase.price\").alias(\"purchase_price\")\n",
    "                    )\n",
    "display(recent_purchases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "607e6599-9a46-4cb6-8654-34558677697e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can also use the getField() method to reference columns inside of structs, here's an example....\n",
    "\n",
    "field_access_df = exploded_purchases_df.select(\n",
    "    \"user_id\",\n",
    "    col(\"purchase\").getField(\"date\").alias(\"purchase_date\"),\n",
    "    col(\"purchase\").getField(\"product_id\").alias(\"product_id\"),\n",
    "    col(\"purchase\").getField(\"name\").alias(\"product_name\"),\n",
    "    col(\"purchase\").getField(\"price\").alias(\"price\")\n",
    ")\n",
    "\n",
    "display(field_access_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64166610-f61b-49b9-aa20-c0afa6b5bb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using the pivot Method\n",
    "\n",
    "The `pivot` method in Spark allows you to transform row data into columnar format, creating a cross-tabulation. This is particularly useful for feature engineering when analyzing categorical data or when you need to reshape your data for reporting or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2592e65-63d5-4ab4-83c1-7783925cc187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's pivot the purchase data to show the count of each product purchased by each user\n",
    "pivot_df = (recent_purchases_df\n",
    "    .groupBy(\"user_id\")\n",
    "    .pivot(\"product_name\")\n",
    "    .agg(count(\"product_id\").alias(\"quantity_purchased\"))\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b2d3e6-78c7-4d44-8264-445e8aee0584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace null values with zeros for better readability\n",
    "pivot_df_no_nulls = (recent_purchases_df\n",
    "    .groupBy(\"user_id\")\n",
    "    .pivot(\"product_name\")\n",
    "    .agg(count(\"product_id\").alias(\"quantity_purchased\"))\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(pivot_df_no_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e075bc-e882-4538-9af2-37f53c7583c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Struct Type Operations**:\n",
    "   - Use dot notation for simple access\n",
    "   - `getField()` for dynamic column access\n",
    "   - Maintain schema clarity\n",
    "\n",
    "2. **Array Operations**:\n",
    "   - Use array functions for manipulation\n",
    "   - Leverage explode for detailed analysis\n",
    "   - Consider performance with large arrays\n",
    "\n",
    "3. **Complex Aggregate Functions**:\n",
    "   - Use the `collect_list` and `collect_set` methods to create arrays from grouped data\n",
    "   - Use the `pivot` function to transform row values into columns for analysis and reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab59c3b-5e34-49b7-af6c-c5f79959099d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Working with Complex Data Types in E-Commerce Data\n",
    "\n",
    "In this lab, you'll practice working with complex data types in Spark, including handling JSON strings, converting them to structured types, and manipulating nested data structures.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are a data engineer at an e-commerce company that collects data about customer orders, product reviews, and customer browsing behavior. The data contains nested structures that need to be properly processed for analysis.\n",
    "\n",
    "### Objectives\n",
    "- Convert JSON string data to Spark SQL native complex types\n",
    "- Work with arrays and structs\n",
    "- Use functions like explode, collect_list, and pivot\n",
    "- Extract and analyze valuable insights from nested data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7cd2ab0-fbff-4b8c-bafb-004ba735c16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b33dfcb4-6f3c-4668-8867-59dea777f769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "# Define our sample e-commerce data with JSON strings\n",
    "data = [\n",
    "    (1001, \"Jordan Smith\", \"jordan.smith@email.com\", \"2022-03-15\",\n",
    "     \"\"\"[\"loyal\", \"premium\", \"tech-enthusiast\"]\"\"\",\n",
    "     \"\"\"[\n",
    "         {\"order_id\": \"O8823\", \"date\": \"2023-01-05\", \"total\": 799.99, \"items\": [\n",
    "           {\"product_id\": \"PHONE-256\", \"name\": \"Smartphone XS\", \"price\": 699.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"CASE-101\", \"name\": \"Phone Case\", \"price\": 29.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"CHGR-201\", \"name\": \"Fast Charger\", \"price\": 49.99, \"quantity\": 1}\n",
    "         ]},\n",
    "         {\"order_id\": \"O9012\", \"date\": \"2023-02-18\", \"total\": 129.95, \"items\": [\n",
    "           {\"product_id\": \"HDPHN-110\", \"name\": \"Wireless Headphones\", \"price\": 129.95, \"quantity\": 1}\n",
    "         ]}\n",
    "       ]\"\"\",\n",
    "     \"\"\"[\"smartphones\", \"accessories\", \"audio\", \"wearables\"]\"\"\"\n",
    "    ),\n",
    "    \n",
    "    (1002, \"Alex Johnson\", \"alex.j@email.com\", \"2021-11-20\",\n",
    "     \"\"\"[\"new\", \"standard\", \"home-office\"]\"\"\",\n",
    "     \"\"\"[\n",
    "         {\"order_id\": \"O8901\", \"date\": \"2023-01-10\", \"total\": 1299.99, \"items\": [\n",
    "           {\"product_id\": \"LAPTOP-15\", \"name\": \"Ultrabook Pro\", \"price\": 1199.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"MOUSE-202\", \"name\": \"Ergonomic Mouse\", \"price\": 49.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"KYBRD-303\", \"name\": \"Mechanical Keyboard\", \"price\": 89.99, \"quantity\": 1}\n",
    "         ]}\n",
    "       ]\"\"\",\n",
    "     \"\"\"[\"laptops\", \"office-equipment\", \"monitors\", \"storage\"]\"\"\"\n",
    "    ),\n",
    "    \n",
    "    (1003, \"Taylor Williams\", \"t.williams@email.com\", \"2022-08-05\",\n",
    "     \"\"\"[\"standard\", \"gamer\"]\"\"\",\n",
    "     \"\"\"[\n",
    "         {\"order_id\": \"O9188\", \"date\": \"2023-02-01\", \"total\": 2099.97, \"items\": [\n",
    "           {\"product_id\": \"GPU-3080\", \"name\": \"Graphics Card RTX\", \"price\": 899.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"CPU-i9\", \"name\": \"Processor i9\", \"price\": 499.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"RAM-32GB\", \"name\": \"Gaming RAM 32GB\", \"price\": 189.99, \"quantity\": 2},\n",
    "           {\"product_id\": \"MBOARD-Z\", \"name\": \"Gaming Motherboard\", \"price\": 319.99, \"quantity\": 1}\n",
    "         ]}\n",
    "       ]\"\"\",\n",
    "     \"\"\"[\"gaming\", \"pc-components\", \"monitors\", \"accessories\"]\"\"\"\n",
    "    ),\n",
    "    \n",
    "    (1004, \"Morgan Lee\", \"morgan.lee@email.com\", \"2022-06-10\",\n",
    "     \"\"\"[\"standard\", \"photography\"]\"\"\",\n",
    "     \"\"\"[\n",
    "         {\"order_id\": \"O9021\", \"date\": \"2023-01-15\", \"total\": 3299.98, \"items\": [\n",
    "           {\"product_id\": \"CAM-DSLR\", \"name\": \"Professional Camera\", \"price\": 2499.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"LENS-50mm\", \"name\": \"Prime Lens\", \"price\": 349.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"TRIPOD-P\", \"name\": \"Premium Tripod\", \"price\": 149.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"SDCARD-128\", \"name\": \"Memory Card 128GB\", \"price\": 79.99, \"quantity\": 3}\n",
    "         ]},\n",
    "         {\"order_id\": \"O9254\", \"date\": \"2023-02-28\", \"total\": 299.98, \"items\": [\n",
    "           {\"product_id\": \"BAG-CAM\", \"name\": \"Camera Bag\", \"price\": 189.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"CLEAN-KIT\", \"name\": \"Lens Cleaning Kit\", \"price\": 29.99, \"quantity\": 1}\n",
    "         ]}\n",
    "       ]\"\"\",\n",
    "     \"\"\"[\"cameras\", \"photography\", \"lenses\", \"accessories\"]\"\"\"\n",
    "    ),\n",
    "    \n",
    "    (1005, \"Casey Rivera\", \"casey.r@email.com\", \"2021-09-30\",\n",
    "     \"\"\"[\"premium\", \"smart-home\"]\"\"\",\n",
    "     \"\"\"[\n",
    "         {\"order_id\": \"O8765\", \"date\": \"2023-01-02\", \"total\": 1029.95, \"items\": [\n",
    "           {\"product_id\": \"SMHUB-01\", \"name\": \"Smart Home Hub\", \"price\": 249.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"SMSPK-02\", \"name\": \"Smart Speaker\", \"price\": 179.99, \"quantity\": 2},\n",
    "           {\"product_id\": \"SMBLB-03\", \"name\": \"Smart Bulbs Pack\", \"price\": 119.99, \"quantity\": 3},\n",
    "           {\"product_id\": \"SMSENS-04\", \"name\": \"Motion Sensors\", \"price\": 89.99, \"quantity\": 1}\n",
    "         ]},\n",
    "         {\"order_id\": \"O9181\", \"date\": \"2023-02-15\", \"total\": 349.98, \"items\": [\n",
    "           {\"product_id\": \"SMDLOCK-05\", \"name\": \"Smart Door Lock\", \"price\": 249.99, \"quantity\": 1},\n",
    "           {\"product_id\": \"SMCAM-06\", \"name\": \"Indoor Camera\", \"price\": 99.99, \"quantity\": 1}\n",
    "         ]}\n",
    "       ]\"\"\",\n",
    "     \"\"\"[\"smart-home\", \"security\", \"automation\", \"speakers\"]\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the schema for the raw data\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"tags\", StringType(), True),\n",
    "    StructField(\"recent_orders\", StringType(), True),\n",
    "    StructField(\"browsing_history\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "ecommerce_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create temporary view\n",
    "ecommerce_df.createOrReplaceTempView(\"ecommerce_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d52deaa-2ecb-47a9-bb12-7d3f9d4e1fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Querying the newly created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe48f28-0374-4e54-8a6d-441caa4d9da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ecommerce_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20517ca0-9f64-4f60-a9fb-87d71111a2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Inspect Raw Data with JSON Strings\n",
    "\n",
    "Load and examine the retail dataset which includes JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0caaa87-2e58-4028-baa2-db49cc942cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Read the sample dataset\n",
    "events_df = spark.read.table(\"ecommerce_raw\")\n",
    "\n",
    "## Examine the schema and display sample data\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33471d31-f8e9-481c-a97b-490ae4cbd12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Convert JSON Strings to Structured Types\n",
    "\n",
    "The `tags`, `recent_orders`, and `browsing_history` columns contain JSON strings. Let's convert them to proper Spark structured types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1dfbf61-bd8a-4bc4-85d6-13e6ed39b82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get a sample of the JSON strings in each column\n",
    "# 2. Infer schemas from the JSON samples\n",
    "# 3. Convert the JSON strings to structured types using from_json and display the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76ecc3f-ef1b-4a2e-a57e-a81430dc1772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Get a sample of the JSON strings\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409818cd-6959-4793-b644-ea94b17a80ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Infer schemas from the JSON samples\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31406250-478f-49cb-9f6b-78b37af30e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert the JSON strings to structured types using from_json and display the resulting DataFrame\n",
    "parsed_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7cf3f44-43f1-4ad9-b442-938eea5fd741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with Arrays\n",
    "\n",
    "Now that we have proper structured data, let's analyze the customer tags and browsing history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291f017b-e98f-41e0-9682-cdb771b36eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Calculate the number of tags and browsing history items for each customer\n",
    "# 2. Explode the tags array to see all unique customer tags\n",
    "# 3. Find the most common browsing categories across all customers\n",
    "# HINT: use the `array_size` function or its alias `size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac2a4836-abd6-493f-9154-956a6371801e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Calculate the number of tags and browsing history items for each customer\n",
    "array_sizes_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f79388e-1352-467b-8dbd-96f8ce3dbbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Explode tags to analyze customer categorization\n",
    "exploded_tags_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2414872c-5604-4f7d-8211-f4588744eb2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Find the most common customer tags\n",
    "tag_counts_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e92a7961-facd-4825-8011-cc6010cb34ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Explode the recent_orders array to analyze individual orders\n",
    "# 2. Calculate total revenue per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33fdc9fd-ab9e-4576-957a-bc949c27050b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Explode the recent_orders array to analyze individual orders\n",
    "orders_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde873bb-b407-41e9-9050-928836c30270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bonus Challenge: Analyze Customer Purchasing Patterns\n",
    "\n",
    "Let's use the `collect_list` and `collect_set` aggregate functions to create summaries of customer purchasing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b93c772f-919f-4648-8cdc-e61f81b917fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## First, create a flattened view of orders\n",
    "order_items_df = orders_df.select(\n",
    "    \"customer_id\",\n",
    "    \"name\",\n",
    "    \"order.order_id\",\n",
    "    \"order.date\",\n",
    "    explode(\"order.items\").alias(\"item\")\n",
    ")\n",
    "\n",
    "## Now extract the name field from each item\n",
    "item_details_df = order_items_df.selectExpr(\n",
    "    \"customer_id\",\n",
    "    \"name\",\n",
    "    \"item.name as product_name\"\n",
    ")\n",
    "\n",
    "# Inspect the data\n",
    "display(item_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6afa52f-6593-4f97-92fe-65f3056e477f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Collect all products purchased by each customer, creating new columns called \"all_products_purchased\" and \"unique_products_purchased\" for each \"customer_id\"\n",
    "customer_products_df = <FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b90f3b2-5bbd-44b4-87f5-6968bfb61901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8d18a8e-0343-4cd2-b192-b764cd0278fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optional - Basic ETL with the DataFrame API\n",
    "\n",
    "This demonstration will walk through common ETL operations using the Flights dataset. We'll cover data loading, cleaning, transformation, and analysis using the DataFrame API.\n",
    "\n",
    "### Objectives\n",
    "- Implement common ETL operations using Spark DataFrames\n",
    "- Handle data cleaning and type conversion\n",
    "- Create derived features through transformations\n",
    "\n",
    "NOTE: This section is optional and should be taught at start to give intro to Dataframe API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3184fdee-3784-4d5b-8ee5-640681a91501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Loading and Inspection\n",
    "\n",
    "First, let's load and inspect the flight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f095a119-d6a0-4dce-9e1f-a5b9cdb436a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the flights data\n",
    "flights_df = spark.read.table(\"databricks_airline_performance_data.v01.flights_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff576e1b-d6d6-4e4a-90bb-7d73073d91b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the schema\n",
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c83487f4-2ca6-4987-bdb0-d88c7b81b7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visually inspect a subset of the data\n",
    "display(flights_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0dd1c39-31f9-4f45-acff-72344966fb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's remove columns we dont need, remember \"filter early, filter often\"\n",
    "flights_required_cols_df = flights_df.select(\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"DayofMonth\",\n",
    "    \"DepTime\",\n",
    "    \"FlightNum\",\n",
    "    \"ActualElapsedTime\",\n",
    "    \"CRSElapsedTime\",\n",
    "    \"ArrDelay\")\n",
    "\n",
    "# Alternatively we could have used the drop() method to remove the columns we didnt want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f95d80-8550-4375-9cf0-67803eda36ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a count of the source data records\n",
    "initial_count = flights_required_cols_df.count()\n",
    "\n",
    "print(f\"Source data has {initial_count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d387a1e7-d1eb-4800-9445-3050e8a5132a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's examine the data for invalid values, these can include nulls or invalid values for string columns \"ArrDelay\", \"ActualElapsedTime\", \"DepTime\" which we intend on performing mathematical opeations on, we can use the Spark SQL COUNT_IF function to perform the analysis\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table with cast columns\n",
    "flights_required_cols_df \\\n",
    "    .selectExpr(\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"TRY_CAST(DepTime AS INT) AS DepTime\",\n",
    "        \"FlightNum\",\n",
    "        \"TRY_CAST(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"TRY_CAST(ArrDelay AS INT) AS ArrDelay\"\n",
    "    ) \\\n",
    "    .createOrReplaceTempView(\"flights_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059a5534-f715-4035-8f5a-b6ce64725f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The flights data contains some invalid and missing values, lets find them and clean them (in this case we will drop them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b038ee86-a45e-454b-93f7-a15305090f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To drop rows where any specified columns are null, we can use the na.drop DataFrame method\n",
    "non_null_flights_df = flights_required_cols_df.na.drop(\n",
    "    how='any',\n",
    "    subset=['CRSElapsedTime']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8d68e8-2cc1-43a9-a234-41e9bc8bfae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Let's remove rows with invalid values for \"ArrDelay\", \"ActualElapsedTime\" and \"DepTime\" columns\n",
    "flights_with_valid_data_df = non_null_flights_df.filter(\n",
    "    col(\"ArrDelay\").try_cast(\"integer\").isNotNull() & \n",
    "    col(\"ActualElapsedTime\").try_cast(\"integer\").isNotNull() &\n",
    "    col(\"DepTime\").try_cast(\"integer\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b612edae-4da1-4f5f-8a93-33ddd5fa341c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now that we know \"ArrDelay\" and \"ActualElapsedTime\" contain integer values only, lets cast them from strings to integers (replacing the existing columns)\n",
    "clean_flights_df = flights_with_valid_data_df \\\n",
    "    .withColumn(\"ArrDelay\", col(\"ArrDelay\").try_cast(\"integer\")) \\\n",
    "    .withColumn(\"ActualElapsedTime\", col(\"ActualElapsedTime\").try_cast(\"integer\"))\n",
    "\n",
    "clean_flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f420a1ca-3166-4139-b202-3568181ef43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Enrichment\n",
    "\n",
    "Now let's create a useful derived column to categorize delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f213031f-e2b6-49ab-a1e6-e55cc3e43339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's start by deriving the \"FlightDateTime\" column from the \"Year\", \"Month\", \"DayofMonth\", \"DepTime\" columns, then drop the constituent columns\n",
    "from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substr, lit, pmod\n",
    "\n",
    "flights_with_datetime_df = clean_flights_df.withColumn(\n",
    "    \"FlightDateTime\",\n",
    "    make_timestamp_ntz(\n",
    "        col(\"Year\"),\n",
    "        col(\"Month\"),\n",
    "        col(\"DayofMonth\"),\n",
    "        pmod(substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(1), lit(2)).try_cast(\"integer\"), lit(24)),\n",
    "        substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(3), lit(2)).try_cast(\"integer\"),\n",
    "        lit(0)\n",
    "    )\n",
    ").drop(\"Year\", \"Month\", \"DayofMonth\", \"DepTime\")\n",
    "\n",
    "# Show the result\n",
    "display(flights_with_datetime_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d37f02-4676-4aa3-ae9a-88ed8b5503a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OK now lets derive the \"ElapsedTimeDiff\" column from the \"ActualElapsedTime\" and \"CRSElapsedTime\" columns\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "flights_with_elapsed_time_diff_df = flights_with_datetime_df.withColumn(\n",
    "    \"ElapsedTimeDiff\", col(\"ActualElapsedTime\") - col(\"CRSElapsedTime\")\n",
    "    ).drop(\"ActualElapsedTime\", \"CRSElapsedTime\")\n",
    "\n",
    "display(flights_with_elapsed_time_diff_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3380c5a3-9d93-4cd1-8deb-6206d5b54eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now lets categorize the \"ArrDelay\" column into categories: \"On Time\", \"Slight Delay\", \"Moderate Delay\", \"Severe Delay\"\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "enriched_flights_df = flights_with_elapsed_time_diff_df \\\n",
    "    .withColumn(\"delay_category\", when(col(\"ArrDelay\") <= 0, \"On Time\")\n",
    "        .when(col(\"ArrDelay\") <= 15, \"Slight Delay\")\n",
    "        .when(col(\"ArrDelay\") <= 60, \"Moderate Delay\")\n",
    "        .otherwise(\"Severe Delay\")) \\\n",
    "       .drop(\"ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b95757-7202-4f63-b1c2-7e2a35e9b847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the result \n",
    "display(enriched_flights_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Developing Applications with Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
