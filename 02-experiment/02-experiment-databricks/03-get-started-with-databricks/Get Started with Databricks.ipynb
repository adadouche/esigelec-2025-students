{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c90bbf4-1181-43aa-9baf-66d78b76affd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Get Started with Databricks\n",
    "\n",
    "This lab provides a comprehensive overview of Databricks modern approach to data warehousing, highlighting how a data lakehouse architecture combines the strengths of traditional data warehouses with the flexibility and scalability of the cloud. You’ll learn about the AI-driven features that enhance data transformation and analysis on the Databricks Data Intelligence Platform. Designed for data warehousing practitioners, this course provides you with the foundational information needed to begin building and managing high-performance, AI-powered data warehouses on Databricks. \n",
    "\n",
    "This lab is designed for those starting out in data warehousing and those who would like to execute data warehousing workloads on Databricks. Participants may also include data warehousing practitioners who are familiar with traditional data warehousing techniques and concepts and are looking to expand their understanding of how data warehousing workloads are executed on Databricks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18989732-3dea-428d-a9ae-2b26a17f27b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using Delta Lake Features with Databricks SQL\n",
    "In this lab, we’ll explore the powerful features of Delta Lake and demonstrate how they enhance data management in a data warehousing context. We’ll start by creating and exploring Delta tables, then dive into key features like Time Travel and Version History.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Explore key features of Delta Lake such as **Time Travel**, **Version History**, and metadata management.\n",
    "- Use SQL commands like `DESCRIBE EXTENDED`, `DESCRIBE HISTORY`, `VERSION AS OF`, and `RESTORE TABLE`.\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings reliability, performance, and ACID transactions to data lakes. In this lab, we will:\n",
    "- Create and explore a Delta table.\n",
    "- Simulate data changes and view metadata and history.\n",
    "- Use Time Travel to query and restore data to previous states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d97763cf-48f9-4760-9937-01071c28fa47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90ccf0d2-4d32-41c5-b066-b734ff963a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create the `retail_sales` table** and view the data in the table\n",
    "- We will use a pre-existing dataset file located at `DA.paths.datasets.retail/source_files/sales.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd153d22-1466-4dda-8ac3-607ece1a3ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a Delta table from the CSV file\n",
    "DROP TABLE IF EXISTS retail_sales;\n",
    "CREATE TABLE IF NOT EXISTS retail_sales\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/databricks_simulated_retail_customer_data/v01/source_files/sales.csv',\n",
    "  format => 'csv',\n",
    "  header => true,\n",
    "  inferSchema => true\n",
    ");\n",
    "-- Display Data of the retail_sales table\n",
    "SELECT * FROM retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b03a02f-e4ee-4021-9d0c-ef285cc8f921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore the Table\n",
    "\n",
    "Delta Lake tables store metadata that can be queried for insights about the table structure, state, and history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94fd0a13-2cfe-4e10-a169-9c363732c315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### View Extended Metadata\n",
    "Use the following command to view detailed metadata about the `retail_sales` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605eeaf7-796a-4145-93b9-472d135b2e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571bf078-2c6a-465d-88a5-a3b706a117ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Describe the History of the Table\n",
    "Run the following command to view the history of `retail_sales`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e63e1d4-550d-485c-9b83-7d157b8a827c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf5c439b-08e9-4b03-b963-5ac3f709022d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As the `retail_sales` table has not been modified yet, its version history will only show **Version 0**, which represents the initial creation of the table. Further modifications to the table will create additional versions in the history log.\n",
    "\n",
    "The history includes detailed metadata about the table's state and modifications:\n",
    "- **Version**: Indicates the specific version of the table.\n",
    "- **Timestamp**: Specifies when the operation occurred.\n",
    "- **Operation**: Describes the type of modification (e.g., `INSERT`, `UPDATE`, `DELETE`).\n",
    "- **Operation Metrics**: Includes key metrics like the number of rows added, removed, or affected, and the total data size.\n",
    "- **User Information**: Tracks which user performed the operation.\n",
    "- **Cluster Information**: Logs the cluster ID where the operation was executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a1565fe-4dca-48c4-970a-ce38a41167ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simulating Data Changes and Time Travel Queries\n",
    "\n",
    "Delta Lake allows for simulating data modifications and querying historical versions of the data using Time Travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ceb61e-de98-488b-93d6-b202972cc8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7e632e2-3ea6-4601-b4e5-8711d842fd24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Simulate Data Changes\n",
    "We will simulate updates and deletions to demonstrate how Delta Lake tracks changes in the table's version history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1045729b-7d71-4def-873e-b179c44562a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Task 1: Update the Table\n",
    "Use the following command to update the `retail_sales` table, updating the `product_name` column for all rows where the `product_category` is `Rony`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786de67e-1399-45bb-919d-1e132bf342d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE retail_sales\n",
    "   SET product_name = 'Updated Items'\n",
    "   WHERE product_category = 'Rony';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76f51f85-7723-46d1-aa73-40fc844a0003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before proceeding, let's obtain a timestamp representing this particular moment in time, which will be useful in a subsequent task. After executing, copy the resulting value to the clipboard for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ca05aa-5c4f-43b9-a0a3-b7e0cf073c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_timestamp();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a28852b-04fa-4a85-a0a6-06ed0ececdfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delete Records\n",
    "Use the following command to delete specific records from the `retail_sales` table, removing all records for a specific customer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695f2c0f-b449-42fd-b14f-ce3a127236f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DELETE FROM retail_sales\n",
    "WHERE customer_name = 'VASQUEZ,  YVONNE M';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c6599ca-aaa2-4dc5-adcd-58543140be18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###View the Table's History\n",
    "\n",
    "To understand the number of versions currently in the table, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540fb1f4-1636-43fd-b085-46a00642e808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f627ec9b-8274-49d1-b832-78860cd2c099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Lake supports various operations that allow robust data management:\n",
    "- **OPTIMIZE**: Optimizes the storage layout of the Delta table for better query performance. This operation compacts smaller files into larger ones, reducing the number of files scanned during queries.\n",
    "- **UPDATE**: Modifies existing records in the table based on a condition.\n",
    "- **DELETE**: Removes specific rows from the table based on a condition.\n",
    "\n",
    "These operations enable efficient data management and ensure the table remains up-to-date with minimal manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ddce9f-6941-476d-affc-f96c74336807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Time Travel Queries\n",
    "\n",
    "Delta Lake's Time Travel feature allows you to query data as it existed in previous versions or at specific timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5d3d16-e2a9-4fb8-a66d-7a60e9b4b442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query a Specific Version\n",
    "Retrieve data from an early version of the table using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ba2f29-dcb4-4f3a-bab8-a5330845e82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "  FROM retail_sales\n",
    "  VERSION AS OF 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5be89c-0f6e-41fa-9a82-b977d1595f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####  Query by Timestamp\n",
    "Retrieve data as it existed at a specific timestamp. Replace the text below with the timestamp copied earlier, uncomment the following lines, and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94bccb24-28e9-4a8a-8b8b-616471e13298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SELECT * \n",
    "--   FROM retail_sales\n",
    "--   TIMESTAMP AS OF 'PASTE TIMESTAMP HERE';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ce1269-8168-475d-a79b-f969a0f057b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Restore Table to a Previous Version\n",
    "\n",
    "Delta Lake provides a powerful feature to restore a table to a previous state. This is particularly useful in scenarios where data is accidentally modified or deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "815fe2e2-8fb4-4252-86a4-5d65bf0b72b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###View Table History\n",
    "Before restoring, check the table's history to identify the version you want to restore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc619e34-c024-4051-8bb0-13cae57b04f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a72144-855e-4fab-99d8-44c122fca89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This command displays the table's operation history, including timestamps and version numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab875317-f999-46a8-830d-4752d7ea95b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Restore the Table\n",
    "Use the following command to restore the `retail_sales` table to a specific version. For this lab, we’ll restore it to **Version 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943f0240-7056-492c-993b-a8d16d84a521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "RESTORE TABLE retail_sales TO VERSION AS OF 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f646a26c-8d00-4aa0-ba83-0db5c3cf067f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This command reverts the table to the specified version, undoing any changes made after that version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02791915-71dd-4a5e-861c-0256e1f6dbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Verify the Restoration\n",
    "After restoring, query the table to confirm that it has been reverted to the expected state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c96c6dd-16ff-495f-98ea-c04205f792ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8ccd56e-e5fd-4f2b-aaee-75c8cb06da0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The table will reflect the data as it existed in **Version 2**. This can also be validated by reviewing the history.\n",
    "Use this feature to safeguard data integrity and recover from unintended modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607a29dc-8fa8-4660-801d-2d114977fcb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY retail_sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb887b2-b89d-428f-8f89-593137cfda19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use this feature to safeguard data integrity and recover from unintended modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a11c80d4-56be-44e1-b04a-b8b6baea5bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "Delta Lake provides powerful features such as **Time Travel**, **Version History**, and **metadata management**, Delta Lake provides robust solutions for querying historical data, tracking changes, and recovering from unintended modifications. These capabilities not only ensure data reliability and consistency but also enhance scalability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f580c78d-a0f0-4685-86fb-42c8d5ef170c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion Techniques\n",
    "\n",
    "This notebook demonstrates the practical application of various data ingestion techniques in the Databricks Lakehouse, including:\n",
    "- **CREATE TABLE AS SELECT** (CTAS)\n",
    "- **COPY INTO** for incremental data loading\n",
    "- Using the **Databricks Upload UI**\n",
    "- Automating real-time ingestion with **Auto Loader**\n",
    "- An introduction to **Lakeflow Connect**\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "- Create and populate Delta tables using **CREATE TABLE AS SELECT (CTAS)**.\n",
    "- Incrementally load data into Delta tables using **COPY INTO**.\n",
    "- Perform manual data ingestion through the **Databricks Upload UI**.\n",
    "- Set up and manage real-time data ingestion pipelines with **Auto Loader**.\n",
    "- Introduction to **Lakeflow Connect** for automated data ingestion pipeline creation and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d646f2-bd24-4f38-93e9-a4621e7672ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Files\n",
    "In the cell below, we are going to run a query on a directory of parquet files. These files are not currently registered as any kind of data object (i.e., a table), but we can run some kinds of queries exactly as if they were. We can run these queries on many data file types, too (CSV, JSON, etc.).\n",
    "\n",
    "Most workflows will require users to access data from external cloud storage locations. \n",
    "\n",
    "In most companies, a workspace administrator will be responsible for configuring access to these storage locations. In this course, we are simply going to use data files that were already set up as part of the lab environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86449ed2-c18d-4b6f-90e7-d73c80ac00e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM parquet.`/Volumes/databricks_simulated_e_commerce_clickstream_data/v01/raw/sales-historical` LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6ddacc-11e7-482a-b1d6-d9009aa7c680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can equivalently use the `read_files()` function to read from files. The syntax is more complicated, but it allows us to pass parameters into the reader which is often required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a206f50-72ac-447a-9646-301da7e89598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM\n",
    "  read_files(\n",
    "    '/Volumes/databricks_simulated_retail_customer_data/v01/source_files/sales.csv',\n",
    "    format => 'csv',\n",
    "    header => true,\n",
    "    inferSchema => true\n",
    "  ) LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7767cd05-fff8-4d9c-bd7c-4b4c890071d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Table as Select (CTAS)\n",
    "\n",
    "We are going to create a table that contains historical sales data from a previous point-of-sale system. This data is in the form of parquet files.\n",
    "\n",
    "**`CREATE TABLE AS SELECT`** statements create and populate Delta tables using data retrieved from an input query. We can create the table and populate it with data at the same time.\n",
    "\n",
    "CTAS statements automatically infer schema information from query results and do **not** support manual schema declaration. \n",
    "\n",
    "This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff7ce41-e722-46cb-896d-f6c91636129c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create or replace the table 'retail_sales_bronze' using Delta format\n",
    "CREATE OR REPLACE TABLE retail_sales_bronze \n",
    "  USING DELTA AS\n",
    "    SELECT * FROM parquet.`/Volumes/databricks_simulated_e_commerce_clickstream_data/v01/raw/sales-historical`;\n",
    "\n",
    "-- Describe the structure of the 'retail_sales_bronze' table\n",
    "DESCRIBE retail_sales_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a87700-3c37-43b6-acab-e20464297d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By running `DESCRIBE <table-name>`, we can see column names and data types. We see that the schema of this table looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7a07ca-063c-449b-9140-69f857d92096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## COPY INTO for Incremental Loading\n",
    "**`COPY INTO`** provides an idempotent option to incrementally ingest data from external sources.\n",
    "\n",
    "Note that this operation does have some expectations:\n",
    "- Data schema should be consistent\n",
    "- Duplicate records should try to be excluded or handled downstream\n",
    "\n",
    "This operation is potentially much cheaper than full table scans for data that grows predictably.\n",
    "\n",
    "We want to capture new data but not re-ingest files that have already been read. We can use `COPY INTO` to perform this action. \n",
    "\n",
    "The first step is to create an empty table. We can then use COPY INTO to infer the schema of our existing data and copy data from new files that were added since the last time we ran `COPY INTO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e662370-e0a2-4e5a-a07d-5d37b7d8d2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS users_bronze;\n",
    "CREATE TABLE users_bronze USING DELTA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73f3114-b858-49c7-bac8-f388f99ec6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**COPY INTO** loads data from data files into a Delta table. This is a retriable and idempotent operation, meaning that files in the source location that have already been loaded are skipped.\n",
    "\n",
    "The cell below demonstrates how to use COPY INTO with a parquet source, specifying:\n",
    "- The path to the data.\n",
    "- The FILEFORMAT of the data, in this case, parquet.\n",
    "- COPY_OPTIONS -- There are a number of key-value pairs that can be used. We are specifying that we want to merge the schema of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413eb54b-ea59-4c07-bdb7-e511a3c4c5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO users_bronze\n",
    "  FROM '/Volumes/databricks_simulated_e_commerce_clickstream_data/v01/raw/users-30m'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bef2593-51ef-42eb-af3a-aeb71b0d1c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## COPY INTO is Idempotent\n",
    "COPY INTO keeps track of the files it has ingested previously. We can run it again, and no additional data is ingested because the files in the source directory haven't changed. Let's run the `COPY INTO` command again to show this. \n",
    "\n",
    "The count of total rows is the same as the `number_inserted_rows` above because no new data was copied into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe958ab-1f52-40a8-8b54-b9e1b640baa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO users_bronze\n",
    "  FROM '/Volumes/databricks_simulated_e_commerce_clickstream_data/v01/raw/users-30m'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "SELECT count(*) FROM users_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55faa687-b971-4018-9adf-ce7d2e8a1387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Built-In Functions\n",
    "\n",
    "Databricks has a vast [number of built-in functions](https://docs.databricks.com/en/sql/language-manual/sql-ref-functions-builtin.html) you can use in your code.\n",
    "\n",
    "We are going to create a table for user data generated by the previous point-of-sale system, but we need to make some changes. \n",
    "\n",
    "The `first_touch_timestamp` is in the wrong format. We need to divide the timestamp that is currently in microseconds by 1e6 (1 million). We will then use `CAST` to cast the result to a [TIMESTAMP](https://docs.databricks.com/en/sql/language-manual/data-types/timestamp-type.html). Then, we `CAST` to [DATE](https://docs.databricks.com/en/sql/language-manual/data-types/date-type.html).\n",
    "\n",
    "Since we want to make changes to the `first_touch_timestamp` data, we need to use the `CAST` keyword. The syntax for `CAST` is `CAST(column AS data_type)`. We first cast the data to a `TIMESTAMP` and then to a `DATE`.  To use `CAST` with `COPY INTO`, we need to use a `SELECT` clause (make sure you include the parentheses) after the word `FROM` (in the `COPY INTO`).\n",
    "\n",
    "Our **`SELECT`** clause leverages two additional built-in Spark SQL commands useful for file ingestion:\n",
    "* **`current_timestamp()`** records the timestamp when the logic is executed\n",
    "* **`_metadata.file_name`** records the source data file for each record in the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76109681-39b7-40bc-b739-25923cc714ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS users_bronze;\n",
    "CREATE TABLE users_bronze;\n",
    "COPY INTO users_bronze FROM\n",
    "  (SELECT *, \n",
    "    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
    "    current_timestamp() updated,\n",
    "    _metadata.file_name source_file\n",
    "  FROM '/Volumes/databricks_simulated_e_commerce_clickstream_data/v01/raw/users-historical/')\n",
    "  FILEFORMAT = PARQUET\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "SELECT * FROM users_bronze LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9157222b-56d2-4c7e-b7b6-e9ae63517240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Upload UI\n",
    "The add data UI allows you to manually load data into Databricks from a variety of sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c95df01-d120-4add-9423-52b36ef40488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Download a data file. For the purposes of this exercise, you may download the **sales.csv** file by following [this link](/ajax-api/2.0/fs/files/Volumes/databricks_simulated_retail_customer_data/v01/source_files/sales.csv). This will download the CSV file to your browser's download folder.\n",
    "- Upload the data file to create a table. In the [Catalog Explorer](/explore/data/workspace/default) (also available from the left sidebar), do the following:\n",
    "   - In the **workspace** catalog, navigate to the **default** schema. \n",
    "   - Select **Create > Create table** from the top-right corner.\n",
    "   - Drop the **sales.csv** you just downloaded into the drop zone (or use the file navigator to find the file in your downloads folder).\n",
    "- Complete the following steps to create the table:\n",
    "   - Under **Table name**, name the table **`retail_sales_ui`**. Note that options are available to configure additional ingestion behavior, although we do not need to change any of these for this exercise.\n",
    "   - Click **Create table** at the bottom of the page to create the table.\n",
    "   - Confirm the table was created successfully. Then close the Catalog Explorer tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f112e1be-7cfa-465b-a90a-3221042a1de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the SHOW TABLES statement to view the available tables in your schema. Confirm that the **`retail_sales_ui`** table has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b15a020-5955-4e3a-940f-de4b218892a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87980b05-5a0a-4eb3-b9ba-9897e3767273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query the table to review its contents.\n",
    "\n",
    "**NOTE**: If you did not name the table as instructed, an error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3182019c-56f6-4f20-9c69-4bce56d0a18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM retail_sales_ui LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be56c6d5-8c07-4e65-8409-3cbab0d2ea70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Auto Loader?\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader/autoloader-edited-anim.gif\" style=\"float:right; margin-left: 10px\" />\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "Auto Loader provides a strong gaurantee when used with a Delta sink (the data will only be ingested once).\n",
    "\n",
    "### How Auto Loader simplifies data ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard at scale. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "  * Using incremental listing (recommended, relies on filename order)\n",
    "  * Leveraging notification + message queue (when incremental listing can't be used)\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "125b2b80-b837-42fc-b4a3-97729121178b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Auto Loader basics\n",
    "Let's create a new Auto Loader stream that will incrementally ingest new incoming files.\n",
    "\n",
    "In this example we will specify the full schema. We will also use `cloudFiles.maxFilesPerTrigger` to take 1 file a time to simulate a process adding files 1 by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0696636-d70f-493c-bfb6-580bfa14d2f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Visualization and Important Notes\n",
    "\n",
    "Once the Auto Loader stream is running, click on the **display_query** link above the visualization (as shown in the image) to monitor metrics like input rate, processing rate, and batch duration.\n",
    "\n",
    "- The **Input vs. Processing Rate** chart shows how records are being ingested and processed over time.\n",
    "- The **Batch Duration** chart indicates the time taken to process each batch of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa237483-a7fc-4bb9-9aba-d2f201b1c7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Use Auto Loader to read the cloud file\n",
    "schema_location = f\"/Volumes/workspace/default/v01/retail_sales_schema\"\n",
    "\n",
    "cloud_dir = f'/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/orders/stream_json/'\n",
    "retail_sales_df = (spark.readStream\n",
    "                   .format(\"cloudFiles\")\n",
    "                   .option(\"cloudFiles.format\", \"json\")\n",
    "                   .option(\"cloudFiles.maxFilesPerTrigger\", \"1\")\n",
    "                   .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "                   .option(\"cloudFiles.schemaLocation\", schema_location)  # Schema location for Auto Loader\n",
    "                   .load(cloud_dir))  # Load the directory containing the CSV file\n",
    "\n",
    "# Display the streaming DataFrame\n",
    "checkpoint_location = f'/Volumes/workspace/default/checkpoint/retail_sales_df'\n",
    "display(retail_sales_df, checkpointLocation = checkpoint_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5656888e-fa98-48a0-8182-e43f9a60cbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_**🚨Important:**_\n",
    "\n",
    "Make sure to **interrupt the cell** after completing the lab. The streaming query will continue running until explicitly interrupted, which could result in unnecessary resource usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c2abc56-9857-4b7a-a607-d11cd4c9b308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakeflow Connect\n",
    "\n",
    "**NOTE: Lakeflow Connect is an advanced feature for automated data pipeline creation.**\n",
    "\n",
    "Lakeflow Connect simplifies the creation and management of data pipelines for efficient ingestion and transformation of data into Delta Lake.\n",
    "\n",
    "![lakeflow_connect.png](https://www.databricks.com/sites/default/files/inline-images/lakeflow-connect-video.gif?v=1718218999)\n",
    "\n",
    "**The key benefits of using Lakeflow Connect are:**\n",
    "- **Automated Pipeline Creation**: Easily configure data ingestion pipelines from various sources into Delta Lake without extensive coding.\n",
    "- **Seamless Integration**: Lakeflow Connect supports multiple data sources and formats, enabling users to unify their data ingestion workflows.\n",
    "- **Built-In Transformation**: Perform data validation, schema enforcement, and enrichment directly within the pipeline configuration.\n",
    "- **Scalable and Reliable**: Designed for large-scale data processing, ensuring high availability and fault tolerance for enterprise workloads.\n",
    "\n",
    "**Lakeflow Connect enables:**\n",
    "- Real-time and batch data ingestion.\n",
    "- Simplified pipeline monitoring and management.\n",
    "- Integration with Delta Lake and Databricks ecosystem tools for optimized data operations.\n",
    "\n",
    "**Documentation Reference**:\n",
    "Learn more about Lakeflow Connect and its capabilities in the [official Databricks documentation](https://docs.databricks.com/en/ingestion/lakeflow-connect/index.html).\n",
    "\n",
    "**NOTE:** Lakeflow Connect is in preview and not yet generally available. Updates will be provided once it becomes widely accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ed7c5a-4413-4475-b7b4-3505b29e4247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered key data ingestion techniques in the Databricks Lakehouse, such as **CTAS**, **COPY INTO**, the **Upload UI**, and **Auto Loader** for incremental ingestion. Additionally, we introduced **Lakeflow Connect** for automated and scalable pipeline creation. These methods ensure efficient, reliable, and consistent data ingestion workflows, meeting the diverse needs of modern data engineering tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41533b8-d9d4-4d56-889f-073eb9a42c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0294acbd-5984-44c1-a5a5-4d6fdb746029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring Data Transformation in Databricks\n",
    "\n",
    "This part of the notebook demonstrates the **Medallion Architecture** for data transformation, using **Materialized Views (MV)** and **Streaming Tables (ST)** in SQL. The pipeline progresses through the **Bronze**, **Silver**, and **Gold** layers, showcasing how to build efficient data pipelines in Databricks.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "- Understand the **Medallion Architecture** and its role in data pipelines.\n",
    "- Declare and configure **DLT** pipelines for automated data processing.\n",
    "- Use **Materialized Views** and **Streaming Tables** for different data transformation workloads.\n",
    "- Enforce data quality with **constraints** in DLT pipelines.\n",
    "- Explore and analyze tables generated by a DLT pipeline using SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a092ad3-c535-4120-90b2-7ffca8a8405d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tables as Query Results\n",
    "\n",
    "DLT adapts standard SQL queries to combine DDL (data definition language) and DML (data manipulation language) into a unified declarative syntax.\n",
    "\n",
    "There are two distinct types of persistent tables that can be created with DLT:\n",
    "\n",
    "* **Materialized View**  \n",
    "Materialized views are refreshed according to the update schedule of the pipeline in which they’re contained. Materialized views are powerful because they can handle any changes in the input. Each time the pipeline updates, query results are recalculated to reflect changes in upstream datasets that might have occurred because of compliance, corrections, aggregations, or general CDC.\n",
    "\n",
    "* **Streaming Tables**  \n",
    "Streaming tables allow you to process a growing dataset, handling each row only once. Because most datasets grow continuously over time, streaming tables are good for most ingestion workloads. Streaming tables are optimal for pipelines that require data freshness and low latency.\n",
    "\n",
    "Note that both of these objects are persisted as tables stored with the Delta Lake protocol (providing ACID transactions, versioning, and many other benefits). We'll talk more about the differences between materialized views and streaming tables later in the notebook.\n",
    "\n",
    "For both kinds of tables, DLT takes the approach of a slightly modified CTAS (create table as select) statement. Engineers just need to worry about writing queries to transform their data, and DLT handles the rest.\n",
    "\n",
    "The basic syntax for a SQL DLT query is:\n",
    "\n",
    "**`CREATE OR REFRESH [STREAMING] TABLE table_name`**<br/>\n",
    "**`AS select_statement`**<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fde221cd-58ec-4430-ad4d-c608db3dccd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore Available Raw Files\n",
    "\n",
    "Complete the following steps to explore the available raw data files that will be used for the DLT pipeline:\n",
    "\n",
    "- Navigate to the available catalogs by selecting the catalog icon directly to the left of the notebook (do not select the **Catalog** text in the far left navigation bar).\n",
    "- Expand the **databricks_simulated_retail_customer_data > v01 > Volumes**.\n",
    "- Expand the volume that contains your **unique username**.\n",
    "- Expand the **stream-source** directory. Notice that the directory contains three subdirectories: **customers** and **orders**.\n",
    "- Expand each subdirectory. Notice that each contains a JSON file (00.json) with raw data. We will create a DLT pipeline that will ingest the files within this volume to create tables and materialized views for our consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "337f37c0-c444-453c-801a-ba3f717f988c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DLT Pipeline: Customer Order Pipeline\n",
    "\n",
    "Check out how to create **Streaming Tables** and **Materialized Views (MV)** for the Medallion Architecture by reviewing the **Customer Order Pipeline** notebook. Follow these steps:\n",
    "\n",
    "- Open the [Customer Order Pipeline]($./Includes/03 - Data Ingestion and Transformation/Pipelines/Customer%20Order%20Pipeline) notebook.\n",
    "   - Do not attempt to execute the code directly. It is intended to be executed within the context of the DLT pipeline workflow.\n",
    "   - Examine how **Streaming Tables** and **Materialized Views** are implemented to process data through the **Bronze**, **Silver**, and **Gold** layers.\n",
    "   - Observe the step-by-step creation and transformation of tables within the Medallion Architecture, including data ingestion, validation, and enrichment techniques.\n",
    "- After reviewing the pipeline notebook and understanding its concepts, close the tab and return to this notebook to proceed with generating the DLT pipeline and completing additional tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4baa801a-0d03-41ca-9c2c-54bedd766a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Display Pipeline Configuration\n",
    "\n",
    "We are going to manually configure a pipeline using the DLT UI. Configuring this pipeline will require parameters unique to a given user. Run the cell to print out values you'll use to configure your pipeline in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47bbcf1-9626-480d-8eb7-8bb6cb117b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pipeline_name = 'Data Transformation Pipeline'\n",
    "pipeline_root = f'{os.getcwd()}/pipelines'\n",
    "catalog_name = 'workspace'\n",
    "schema_name = 'default'\n",
    "notebook_path = f'{os.getcwd()}/pipelines/00 - Customer Order Pipeline'\n",
    "src_dir = '/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline'\n",
    "\n",
    "\n",
    "print(f\"Pipeline Name:         {pipeline_name}\")\n",
    "print(f\"Pipeline root folder:  {pipeline_root}\")\n",
    "print(f\"Default Catalog:       {catalog_name}\")\n",
    "print(f\"Default Schema:        {schema_name}\")\n",
    "print(f\"Source:                {src_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37d4e6a6-7607-46f3-9a4e-eb2102f3fbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create and Configure a Pipeline\n",
    "\n",
    "Complete the following to configure the pipeline.\n",
    "\n",
    "Steps:\n",
    "- Open the [Pipelines user interface](/pipelines) (or use the **Jobs & Pipelines** option from the left sidebar and select the Pipelines tab).\n",
    "- Click **Create** in the upper-right corner, and select **ETL pipeline** from the dropdown menu.\n",
    "- Set the pipeline name to **Data Transformation Pipeline**.\n",
    "- Then select **Add an existing asset** \n",
    "- Set the **Pipeline Root Folder path** and **Source code paths** using the value **Pipeline root folder** value from above. \n",
    "- Click on **Add** to confirm the pipeline creation.\n",
    "\n",
    "- In the upper right bar click on the **Settings** button. Configure the pipeline as specified below. You'll need the values provided in the cell output above for this step.\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Pipeline name | Enter the **Pipeline Name** provided above |\n",
    "| Pipeline mode | Choose **Triggered** |\n",
    "| Default catalog | Choose your **Default Catalog** provided above |\n",
    "| Default schema | Choose the **Default Schema** provided above |\n",
    "| Pipeline user mode | **Development** |\n",
    "\n",
    "- In the setting, click **Add Configuration** and input the Key and Value in the table below:\n",
    "\n",
    "| Key                 | Value                                      |\n",
    "| ------------------- | ------------------------------------------ |\n",
    "| **`source`** | Enter the **source** provided above |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38831e01-8f33-4bf7-bee2-9b82f932fdd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Your Pipeline Configuration\n",
    "\n",
    "- In the Databricks workspace, open the **[Jobs & Pipeline](/jobs)** UI.\n",
    "- Select the **Data Transformation Pipeline** pipeline configuration created previously.\n",
    "- Review the pipeline configuration settings to ensure they are correctly configured according to the provided instructions.\n",
    "- Once you've confirmed that the pipeline configuration is set up correctly, proceed to the next steps for running the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8441b54-64b0-4e7e-aa51-4680ef0fff5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update the pipeline\n",
    "\n",
    "Trigger an update of the pipeline you created by clicking the **Run Pipeline** button in the Pipeline user interface.\n",
    "\n",
    "You should se your pipeline being executed:\n",
    "\n",
    "![execute](./images/01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f965cd-55c4-4781-9b3c-590db9447689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Querying Tables in the Target Database\n",
    "\n",
    "As long as a target database is specified during DLT Pipeline configuration, tables should be available to users throughout your Databricks environment. Let's explore them now. \n",
    "\n",
    "Run the cell below to see the tables registered to the database used so far. The tables were created in the **dbacademy** catalog, within your unique **schema** name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21293e46-13cb-473c-8fdc-7aebf5263156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21de084e-5ea0-45b0-905b-ba5085a25f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that the view we defined in our pipeline is absent from our tables list.\n",
    "\n",
    "Query results from the **`order_silver`** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93722749-3f33-4e16-88fb-e17fb9f8bb2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM order_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa474882-ce36-4a3e-86a9-0205fede3070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Recall that **`orders_bronze`** was defined as a streaming table in DLT, but our results here are static.\n",
    "\n",
    "Because DLT uses Delta Lake to store all tables, each time a query is executed, we will always return the most recent version of the table. But queries outside of DLT will return snapshot results from DLT tables, regardless of how they were defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81dd2956-1a56-4466-ae0a-9bb530a2510a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Show Lineage for Delta Tables in Unity Catalog\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/uc/lineage/uc-lineage-slide.png\" style=\"float:right; margin-left:10px\" width=\"700\"/>\n",
    "\n",
    "\n",
    "Unity Catalog captures runtime data lineage for all **table-to-table operations** executed on Databricks clusters or SQL endpoints. Lineage works seamlessly across all languages, including **SQL, Python, Scala, and R**. It can be visualized in **Data Explorer** in near real-time and can also be retrieved programmatically using the **REST API**. \n",
    "\n",
    "**Lineage Granularity Levels**\n",
    "\n",
    "Unity Catalog supports data lineage at two levels:\n",
    "- **Table-Level Lineage**:\n",
    "   - Tracks the flow of data between entire tables.\n",
    "   - Useful for understanding the broader context of data operations.\n",
    "\n",
    "- **Column-Level Lineage**:\n",
    "   - Tracks data transformations at the column level.\n",
    "   - Ideal for use cases like **GDPR compliance** and tracking sensitive data dependencies.\n",
    "\n",
    "**Access Control with Table ACLs**\n",
    "\n",
    "Lineage respects the **Table ACLs** (Access Control Lists) defined in Unity Catalog:\n",
    "- If a user does not have access to a table in the lineage graph, its details will be redacted.\n",
    "- However, users will still see the presence of upstream or downstream dependencies, ensuring visibility into the flow of data while maintaining security.\n",
    "\n",
    "---\n",
    "\n",
    "**Benefits of Viewing Lineage**\n",
    "- **End-to-End Data Visibility**:\n",
    "   - Understand how data flows through the pipeline, from source to final output.\n",
    "\n",
    "- **Compliance and Governance**:\n",
    "   - Ensure GDPR compliance by tracking sensitive data dependencies at the column level.\n",
    "\n",
    "- **Debugging and Optimization**:\n",
    "   - Identify bottlenecks and optimize transformations for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669159ee-e386-49ca-ac9a-4262f22898b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Steps to View Lineage in Unity Catalog\n",
    "\n",
    "Follow these steps to view the lineage of Delta Tables in Unity Catalog:\n",
    "\n",
    "**Step 1: Navigate to the Pipelines**\n",
    "- Open the **[Jobs & Pipeline](/jobs)** UI\n",
    "- Select the Data Transformation Pipeline pipeline configuration created previously.\n",
    "\n",
    "**Step 2: Select the Materialized View**\n",
    "- On the pipeline page, locate the materialized view of interest (e.g., `customer_order`).\n",
    "- Click on the materialized view to open its **Details** tab. \n",
    "\n",
    "**Step 3: Open the Table in Catalog Explorer**\n",
    "- Under the **Details** tab of the materialized view, locate the table name (e.g., `workspace.default.customer_order`).\n",
    "- Click on the table name to navigate to the **Catalog Explorer**. \n",
    "\n",
    "**Step 4: View Lineage Tab**\n",
    "- In the Catalog Explorer, select the **Lineage** tab from the menu at the top. \n",
    "- This will display a summary of the table's upstream and downstream dependencies.\n",
    "\n",
    "**Step 5: Open the Lineage Graph**\n",
    "- In the **Lineage** tab, locate the **\"See Lineage Graph\"** button in the top-right corner of the page.\n",
    "- Click on the button to open the expanded lineage graph. \n",
    "\n",
    "**Step 6: Explore the Lineage Graph**\n",
    "- The **Lineage Graph** will display the data flow for the table:\n",
    "   - **Upstream Tables**: Represent the data sources feeding into the pipeline.\n",
    "   - **Downstream Tables**: Represent the outputs or dependencies created from the table.\n",
    "- Click on the **`+` icons** to expand the graph and reveal additional details about each connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab44f20-80e1-4e8d-b400-240415832ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored data transformation in Databricks using the Medallion Architecture, highlighting the capabilities of DLT to build robust and efficient pipelines. We demonstrated the creation and configuration of pipelines that use Materialized Views (MV) and Streaming Tables (ST) to process and transform data across Bronze, Silver, and Gold layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1ccdfa1-97f5-4f9f-b336-35427928367c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43ce144-61a1-4efd-85fe-910560379fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Orchestration and Querying Capabilities\n",
    "\n",
    "In this lab, we’ll set up a **Databricks Lakeflow Jobs** leveraging SQL tasks to automate a series of **Data Warehousing** tasks. These tasks include data ingestion, validation, transformation, and generating insights within a **Medallion Architecture** (Bronze, Silver, Gold layers). Additionally, we will explore error handling, retries, scheduling options, and integration with external tools.\n",
    "\n",
    "![medallion_architecture](./images/medallion_architecture.png)\n",
    "\n",
    "Steps of the Medallion Architecture:\n",
    "\n",
    "- Ingest all CSV files from the **myfiles** volume and create a bronze table.\n",
    "- Prepare the bronze table by adding new columns and create a silver table.\n",
    "- Create a gold aggregated table for consumers.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will learn how to:\n",
    "\n",
    "- Create a Lakeflow Job with **SQL tasks** for a Medallion Architecture pipeline.\n",
    "- Set up task dependencies and implement conditional logic for lakeflow jobs control.\n",
    "- Use the Lakeflow Job UI for **monitoring** and **data lineage visualization** to trace data transformations and dependencies.\n",
    "- Configure error handling and retries for tasks.\n",
    "- Schedule Lakeflow Job using manual triggers.\n",
    "- Set up **notifications** for monitoring and analyze the execution history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fb6639-ede9-4e6e-89e0-cb003b32b747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create ETL Pipelines using the Databricks Python API\n",
    "\n",
    "Instead of using the UI, you can leverage the Databricks APis to create or manage your Databricks assets.\n",
    "\n",
    "In the next steps, you will create the required pipelines to continue the lab instead of using the UI.\n",
    "\n",
    "This is very useful when you want to scale or automate your operations, giving you more flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63992b2b-b673-42fc-864e-2286681829b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8941dde1-7b68-469a-93c0-d9d4f591df0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's first add a helper function that wraps the use of the Databricks API to cretae a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258c7fd5-f819-463d-affd-84daeae8d7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "# /Workspace/Users/adadouche@hotmail.com/esigelec-2025/02-tp/03-databricks/03-get-started-with-databricks-for-data-warehousing/Get Started with Databricks for Data Warehousing\n",
    "# /Workspace/Users/adadouche@hotmail.com/esigelec-2025/02-tp/03-databricks/03-get-started-with-databricks-for-data-warehousing/Notebooks/Pipelines/01 - Raw Data to Bronze\n",
    "# /Workspace/Users/adadouche@hotmail.com/esigelec-2025/02-tp/03-databricks/03-get-started-with-databricks-for-data-warehousing/01 - Raw Data to Bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cacf5c9-d928-4e19-a632-00ff2b560bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import pipelines\n",
    "\n",
    "client = WorkspaceClient()\n",
    "\n",
    "workspace_url = f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}\"\n",
    "\n",
    "def generate_pipeline(\n",
    "    pipeline_name, \n",
    "    pipeline_root_folder, \n",
    "    pipeline_notebooks, \n",
    "    use_catalog, \n",
    "    use_schema, \n",
    "    use_configuration=None, \n",
    "    use_serverless=True, \n",
    "    use_continuous=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a Databricks pipeline based on the specified configuration parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the current notebook folder path\n",
    "    # current_folder_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "    # main_course_folder_path = \"/Workspace\" + \"/\".join(current_folder_path.split(\"/\")[:-1])\n",
    "\n",
    "    # Create paths for the specified notebooks\n",
    "    notebooks_paths = [\n",
    "        f\"{pipeline_root_folder}/{notebook}\" for notebook in pipeline_notebooks\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline_info = client.pipelines.create(\n",
    "        allow_duplicate_names=True,\n",
    "        name=pipeline_name,\n",
    "        catalog=use_catalog,\n",
    "        target=use_schema,\n",
    "        serverless=use_serverless,\n",
    "        continuous=use_continuous,\n",
    "        development=True,  # Development mode\n",
    "        configuration=use_configuration,\n",
    "        libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=path)) for path in notebooks_paths]\n",
    "    )\n",
    "\n",
    "    # Store the pipeline ID\n",
    "    current_pipeline_id = pipeline_info.pipeline_id\n",
    "    print(f\"Successfully created the ETL pipeline '{pipeline_name}' (id: {pipeline_info.pipeline_id}, url : {workspace_url}/pipelines/{pipeline_info.pipeline_id})\")\n",
    "\n",
    "    # https://dbc-173ef31c-a9d4.cloud.databricks.com/pipelines/79127095-d478-4081-a2c0-d5a1350d089c?o=1047293490737669\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c67409-01bd-4ad8-bff2-69ec3137dc65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_list = [\n",
    "    \"01 - Raw Data to Bronze\",\n",
    "    \"02 - Bronze to Silver\",\n",
    "    \"03 - Silver to Gold\"\n",
    "]\n",
    "\n",
    "for pipeline in pipeline_list:\n",
    "    generate_pipeline(\n",
    "        pipeline_name=pipeline,\n",
    "        pipeline_root_folder=f'{os.getcwd()}/resources',\n",
    "        pipeline_notebooks=[\n",
    "            pipeline\n",
    "        ],\n",
    "        use_catalog=\"workspace\", \n",
    "        use_schema=\"default\",\n",
    "        use_configuration={'source': f'/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca777ed-7a92-416a-911b-6ddbda4cbc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Lakeflow Job in the UI\n",
    "\n",
    "- In your Databricks workspace, click on the **Jobs & Pipeline** icon in the left sidebar.\n",
    "   \n",
    "- Click on **Create** in the upper-right corner of the **Jobs & Pipeline** page and select **Job**.\n",
    "\n",
    "- Name the job \"Serverless lakeflow job\" or something similar for easy identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee6b9a9-5670-44c4-aa60-d4a89becfef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add Tasks to the Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35e1d117-8060-46a7-9ac8-b001b0ff6cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add Tasks to the Lakeflow Jobs\n",
    "\n",
    "**Create Your First Task**:\n",
    "   - Name the task `01_Raw_Data_to_Bronze` (spaces are not supported).\n",
    "   - Set **Type** to `Pipeline`.\n",
    "   - **Pipeline** should be set to `01 - Raw Data to Bronze`.\n",
    "   - Click **Create Task**.\n",
    "   - Notifications:\n",
    "      - Add notification to send emails on failure (e.g., ` your-email@databricks.com`).\n",
    "\n",
    "This task runs the Raw Data-to-Bronze pipeline to ingest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faeefac3-9d1e-4770-a4fb-6b928c491b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set Conditional Check for Bronze Pipeline\n",
    "\n",
    "**Create Conditional Task**:\n",
    "   - Click on **Add Task**\n",
    "   - Set **Type** to `If/else condition`.\n",
    "   - Name the task `pipeline_condition_1`.\n",
    "   - Set **Depends on** to `01__Raw_Data_to_Bronze_DLT_Pipeline` to ensure this task runs after data quality checks.\n",
    "   - Condition: Set the expression to:\n",
    "     - Left operand: **&lcub;&lcub;tasks.01_Raw_Data_to_Bronze.result_state&rcub;&rcub;**\n",
    "     - Operator: **==**\n",
    "     - Right operand: **success**\n",
    "   - Click **Save Task**.\n",
    "\n",
    "This task evaluates whether the Bronze pipeline execution succeeded or failed, determining the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b0f95e-954f-4fe5-b2ca-19423195ae33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze to Silver DLT Pipeline\n",
    "\n",
    "**Create Your Second Task**:\n",
    "   - Click on **Add Task**\n",
    "   - Name the task `02_Bronze_to_Silver`.\n",
    "   - Set **Type** to `Pipeline`.\n",
    "   - **Pipeline** should be set to `02 - Bronze to Silver`.\n",
    "   - Set **Depends** on to **`pipeline_condition_1(True)`**\n",
    "   - Click **Create Task**.\n",
    "   - Notifications:\n",
    "      - Add notification to send emails on failure (e.g., ` your-email@databricks.com`).\n",
    "\n",
    "This task processes data from Bronze to Silver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2ac83f-1f21-4ed1-aee6-c26cc13f40c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set Conditional Check for Silver Pipeline\n",
    "\n",
    "**Create Conditional Task**:\n",
    "   - Click on **Add Task**\n",
    "   - Set **Type** to `If/else condition`.\n",
    "   - Name the task `pipeline_condition_2`.\n",
    "   - Set **Depends on** to `02_Bronze_to_Silver`.\n",
    "   - Condition: Set the expression to:\n",
    "     - Left operand: **&lcub;&lcub;tasks.02_Bronze_to_Silver.result_state&rcub;&rcub;**\n",
    "     - Operator: **==**\n",
    "     - Right operand: **success**   \n",
    "   - Click **Save Task**.\n",
    "\n",
    "This task evaluates whether the Silver pipeline execution succeeded or failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc340bc-03ec-4edf-904a-0f049c1f129d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver to Gold DLT Pipeline\n",
    "\n",
    "**Create Your Third Task**:\n",
    "   - Click on **Add Task**\n",
    "   - Name the task `03_Silver_to_Gold`.\n",
    "   - Set **Type** to `Pipeline`.\n",
    "   - **Pipeline** should be set to `03 - Silver to Gold`.\n",
    "   - Set **Depends on** to:\n",
    "     - `pipeline_condition_2 (True)`.\n",
    "   - Notifications:\n",
    "      - Add notification to send emails on failure (e.g., ` your-email@databricks.com`).\n",
    "   - Click **Create Task**.\n",
    "\n",
    "This task processes data from Silver to Gold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14970f19-b688-4d7c-a9f3-dec8fd5e3f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Troubleshooting Notebook\n",
    "\n",
    "**Create Fourth Task**:\n",
    "   - Click on **Add Task**\n",
    "   - Set **Type** to `Notebook`.\n",
    "   - Name the task `Troubleshooting`.\n",
    "   - **Source** should be set to `Workspace`.\n",
    "   - Set **Path** to the notebook for saving the final report (e.g., `./resources/04 - Troubleshooting`).\n",
    "   - Use the same cluster as the previous tasks.\n",
    "   - Set **Depends on** to both:\n",
    "     - `pipeline_condition_1 (False)` and `pipeline_condition_2 (False)`.\n",
    "   - Set **Run if dependencies** to \"At least one succeeded\" to ensure it saves the report regardless of the path taken.\n",
    "   - Notifications:\n",
    "      - Add notification to send emails on Success (e.g., ` your-email@databricks.com`).\n",
    "   - Click **Create Task**.\n",
    "\n",
    "This task runs a troubleshooting notebook to analyze and resolve pipeline issues. Example steps in the notebook include querying logs and providing remediation suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e747184d-2c67-4d3c-bcdf-fc2b047be224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enable Email Notifications\n",
    "\n",
    "- **Set up Notifications**:\n",
    "   - In the job's configuration, navigate to the **Notifications** section.\n",
    "   - Enable email notifications by adding your email to receive updates on job completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "543a1acb-5489-4788-869f-599fa0cdea1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trigger the Lakeflow Jobs Manually\n",
    "\n",
    "Go to the [Jobs & Pipeline](/jobs) in the Databricks UI, select the **Serverless lakeflow job** job and click on **Run Now** in the top-right corner to manually trigger the job. \n",
    "\n",
    "This will execute all tasks in the Lakeflow Job according to their dependencies and conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "282b7f20-f0c0-4e32-884a-3a245850d9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitor the Lakeflow Jobs Execution\n",
    "\n",
    "- **Navigate to the Runs Tab**:\n",
    "   - In the job interface, you can view active and completed executions of the job.\n",
    "   - Select the current execution, which should take you to the list of tasks included in the job\n",
    "\n",
    "- **Observe Task Execution**:\n",
    "   - Each task’s status is displayed, where you can see which tasks are currently executing or have completed.\n",
    "   - Click on each task to view its execution details and outputs, allowing you to troubleshoot and verify each stage.\n",
    "   - Check the logs to see if the Lakeflow Jobs followed the correct path based on the unusual pattern detection condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceb966dd-9a66-4518-b3d3-7ceed9130e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Lineage: Viewing Data Lineage for a Table\n",
    "Data lineage in Unity Catalog provides end-to-end visibility into how data is sourced, transformed, and consumed. With lineage information, you can:\n",
    "\n",
    "- Understand the dependencies of your datasets.\n",
    "- Identify the upstream and downstream impact of schema changes.\n",
    "- Debug pipeline issues by tracing data flow through the system.\n",
    "- Ensure compliance by auditing data usage and transformations.\n",
    "\n",
    "**Benefits of Data Lineage**\n",
    "- **Visibility:** Gain a comprehensive view of data flow across your pipeline.\n",
    "- **Impact Analysis:** Determine how changes in one dataset affect downstream applications.\n",
    "- **Governance and Compliance:** Track data transformations and usage for regulatory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82e5dafd-82a6-4c1b-b306-650e2297b314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Viewing Lineage in Unity Catalog\n",
    "The following code helps you access the lineage information for a table directly in the Databricks UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af41298d-60ad-4ebf-9554-846ec2a46b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate the workspace URL dynamically\n",
    "workspace_url = f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}\"\n",
    "\n",
    "# Define the table name for which to view the lineage\n",
    "table_name = \"order_table_gold\"  # Replace with any other table name as needed\n",
    "\n",
    "# Construct the URL for the data lineage page in Unity Catalog\n",
    "lineage_url = f\"{workspace_url}/explore/data/workspace/default/{table_name}?activeTab=lineage\"\n",
    "\n",
    "# Print a user-friendly message with the lineage URL\n",
    "print(f\"Access the data lineage for the table '{table_name}' using the following URL:\")\n",
    "\n",
    "# Display the URL as a clickable link in Databricks\n",
    "displayHTML(f'<a href=\"{lineage_url}\" target=\"_blank\">Click here to view the lineage for {table_name}</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3b4acd-d1d3-4d1d-a1ea-4735b15402f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you learned how to:\n",
    "- Configure and execute a Databricks Lakeflow job with multiple tasks.\n",
    "- Use dependencies and conditional paths to control the flow of tasks based on the conditions.\n",
    "- Set up email notifications to stay updated on job execution.\n",
    "- Trigger the Lakeflow job manually and monitor its execution.\n",
    "\n",
    "This Lakeflow Jobs setup ensures robust automation for DLT pipelines with integrated troubleshooting and notification mechanisms. The conditional paths provide flexibility to handle success and failure scenarios efficiently, while monitoring and logging enhance visibility into pipeline executions.\n",
    "\n",
    "Additionally, you explored how to leverage **data lineage** within Unity Catalog, enabling deeper insights into the relationships between datasets and transformations. This feature enhances governance, auditing, and troubleshooting across your Lakeflow Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84dbcc52-1da8-4101-88c9-d209fe90bcfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1ca964a-6b76-4262-b894-a7d8d6219eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Presentation with AI/BI Dashboard\n",
    "\n",
    "Databricks AI/BI Dashboards offers an enhanced visualization library and a streamlined configuration experience to help you quickly transform data into shareable insights. \n",
    "\n",
    "In this lab, we will create a new dashboard and add data and visualizations to the dashboard based on table data and SQL queries.\n",
    "\n",
    "This lab uses the following resources from  `databricks_simulated_retail_customer_data.v01`:\n",
    "* **sales** table\n",
    "* **customers** table\n",
    "\n",
    "These tables contain some retail sales figures and customer order details. We'll be using them as the source data for the visualizations in the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2370c124-2d2f-47ed-a8f7-19eefc725b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a new Dashboard\n",
    "Creating a new dashboard in Databricks is simple and straight forward. \n",
    "* Navigate to **Dashboards** in the side navigation pane.\n",
    "* Select **Create dashboard**. \n",
    "* At the top of the resulting screen, click on the Dashboard name and change it to **Retail Dashboard**.\n",
    "\n",
    "You also have the option to import a dashboard if you already have one. All your existing Dashboards can be located from this area of the platform. There are also many quick create features throughout the platform that offer **Dashboards** as one of the options for creating them from other submenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9d0bdd-f06e-450f-ad20-bfab37ce71cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding Data Sources\n",
    "\n",
    "With a completely new dashboard, you'll need to associate the dashboard to data before you can begin. \n",
    "\n",
    "You'll notice at the top of the dashboard screen, you have two tabs, **Page** and **Data**. \n",
    "\n",
    "- **Page:** The Pages tab allows users to create visualizations and construct their dashboards. Each item on the canvas is called a widget. Widgets have three types: visualizations, text boxes, and filters.\n",
    "\n",
    "- **Data:** The **Data** tab allows you to define datasets that you will use in the dashboard. Datasets are bundled with dashboards when sharing, importing, or exporting them using the UI or API.\n",
    "\n",
    "Select the **Data** tab to get started. \n",
    "\n",
    "There are three icons on the left side of the screen: **Dataset list**, **Catalog**, and **Assistant**. \n",
    "* **Dataset list** will present you with a list of all the Datasets and queries used for the dashboard. You can use multiple datasets in a single dashboard which can be selected from the list of available tables or created from SQL queries. \n",
    "* **Catalog** allows you to navigate the available catalogs, schemas, and tables.\n",
    "* **Assistant** provides you with a AI-powered interface for asking queries in natural language to the platform to discover objects or gain insights or assistance on query writing. \n",
    "\n",
    "The following steps walk you through adding the tables for this example dashboard.\n",
    "\n",
    "- With the **Datasets list** icon selected, click the **+ Add Data Source** button.\n",
    "- If needed, select **All**.\n",
    "- From the resulting pop-up, search for `databricks_simulated_retail_customer_data.v01.sales`.\n",
    "- Click **sales** to add it as a dataset, and then select the **Confirm** button. Note that it appears in your dataset list\n",
    "- Repeat these steps to add the `customers` tables.\n",
    "\n",
    "Note that each table is added to the list as an automatically populated `SELECT *` statement in the query editing panel. You can modify the SQL query to alter the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08ee2327-c3dc-4ad3-a47a-feb548c8c697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Adding Visualizations\n",
    "\n",
    "#### Counter\n",
    "\n",
    "The first visualization we'll be adding to the dashboard is a counter visualization to display the current sales against a sales goal of $3 million.\n",
    "\n",
    "- In the **Data** tab, select the **+ Create from SQL** option.\n",
    "- Enter the following query into the query editing space:<br>\n",
    "    ```\n",
    "    SELECT \n",
    "    sum(total_price) AS Total_Sales, \n",
    "    3000000 AS Sales_Goal \n",
    "    FROM databricks_simulated_retail_customer_data.v01.sales;\n",
    "    ```\n",
    "\n",
    "- Click **Run** to execute the query.\n",
    "- Right-click the query in the **Datasets** list and select **Rename**, or use the kebab menu, to rename the query as **Count Total Sales**.\n",
    "\n",
    "Now, let's add our first visualisation. Switch to the **Untitled Page** tab.\n",
    "\n",
    "- At the bottom of the screen you have a toolbar for moving objects, adding a visualization, adding a text box, and adding a filter. Select **Add a visualization**.\n",
    "- Move your cursor to anywhere on the screen and click to add the visualization to the canvas.\n",
    "- In the **Configuration Panel** on the right, make the following selections for the settings:\n",
    "    - **Dataset:** Count Total Sales\n",
    "    - **Visualization:** Counter\n",
    "    - **Title:** Checked\n",
    "      - Click on **Widget Title** on the visualization.\n",
    "      - Change it to **Sales Goal**.\n",
    "    - **Value:** Total_Sales\n",
    "    - **Target:** Sales_Goal\n",
    "\n",
    "- Click on **Total_Sales** in the **Value** area of the configuration panel and select **Format** from the resulting dropdown. Make the following adjustments:\n",
    "    - Change **Auto** to **Custom**\n",
    "    - Set **Type** to Currency ($)\n",
    "    - Set **Abbreviation** to **None**\n",
    "- In the Style section, click the **+** next to **Conditional Style**. Configure it with the following settings:\n",
    "    - If Value <= Target\n",
    "    - Then (Color: Red)\n",
    "\n",
    "This is a really simple visualization but let's you get a feel for working with Visualizations on dashboards. You can adjust the placement and size of the visualization by dragging the edges or click-holding while hovering over the visualization box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0484580c-1c4b-4ec1-9ee1-fc6e5bba8663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adding a Text Box\n",
    "\n",
    "Let's add a name and a space for a text description of the dashboard to the canvas. When adding a new widget to the canvas, other widgets automatically move to accommodate your placement. You can use your mouse to move and resize widgets. To delete a widget, select it and then press the delete key. You can also manipulate the widgets through the use of the kebab menu icon in the upper right corner of each individual one.\n",
    "\n",
    "Complete the following steps to add a text box to the dashboard:\n",
    "\n",
    "-  Click the <b>Add a text box</b> icon and drag the widget to the top of your canvas.\n",
    "- Type: `# Retail organization`\n",
    "\n",
    "    **Note:** Text boxes use markdown. The `#` character in the included texts indicates that <b>Retail organization</b> is a level 1 heading. See <a href=\"https://www.markdownguide.org/basic-syntax/\" target=\"_blank\">this markdown guide</a> for more on basic markdown syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af41c640-3437-4bd2-8d89-fbf9a3e36d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Publishing and Sharing\n",
    "\n",
    "When your dashboard is complete, to share it with others, you need to publish it. \n",
    "\n",
    "Published dashboards can be shared with other users in your workspace and with users registered at the account level. That means that users registered to your Databricks account, even if they have not been assigned workspace access or compute resources, can be given access to your dashboards.\n",
    "\n",
    "When you publish a dashboard, the default setting is to **embed credentials**. Embedding credentials in your published dashboard allows dashboard viewers to **use your credentials to access the data and power the queries that support it**. If you choose not to embed credentials, dashboard viewers use their own credentials to access necessary data and compute power. If a viewer does not have access to the default SQL warehouse that powers the dashboard, or if they do not have access to the underlying data, _visualizations will not render._\n",
    "\n",
    "To publish your dashboard, complete the following steps:\n",
    "\n",
    "- Click <b>Publish</b> in the upper-right corner of your dashboard. Read the setting and notes in the <b>Publish</b> dialog.\n",
    "- Click <b>Publish</b> in the lower-right corner of the dialog. The <b>Sharing</b> dialog should open afterward. If it does not open, you can select **Share** next to **Publish** at the top of the dashboard.\n",
    "    - You can use the text field to search for individual users, or share the dashboard with a preconfigured group, like <b>Admins</b> or <b>All workspace users</b>. From this window, you can grant leveled privileges like <b>Can Manage</b> or <b>Can Edit</b>. See <a href=\"https://docs.databricks.com/en/security/auth-authz/access-control/index.html#lakeview\" target=\"_blank\">Dashboard ACLs</a> for details on permissions.\n",
    "    - The bottom of the <b>Sharing</b> dialog controls view access. Use this setting to easily share with all account users.\n",
    "-  Under <b>Sharing settings</b>, choose <b>Anyone in my account can view</b> from the drop-down. Then, close the <b>Sharing</b> dialog.\n",
    "-  Use the drop-down near the top of the dashboard to switch between <b>Draft</b> and <b>Published</b> versions of your dashboard.\n",
    "\n",
    "**Note:** When you edit your draft dashboard, viewers of the published dashboards do not see your changes until you republish. The published dashboard includes visualizations that are built on queries that can be refreshed as new data arrives. Dashboards are updated with new data automatically without republishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e819bd4-47ab-49e5-b8ed-ce489cfd0dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Interactive Features: Field Filters (Optional)\n",
    "The dashboard you created in the lab is good for reporting, and viewers can use it to stay up-to-date on the most recent retail sales figures. However, the viewer has no controls that allow them to further explore the data. For example, if a user wants to see the data for a specific period, they would need to contact the dashboard author to request any changes.\n",
    "\n",
    "You can create user controls that allow the viewer to filter certain data based on a field or a parameter value. Filters are widgets that allow dashboard viewers to narrow down results by filtering on specific fields or setting dataset parameters. \n",
    "\n",
    "Filters can be applied to fields of one or more datasets. Filters on fields allow users to focus on certain values, or ranges of values in the data. The filter applies to all visualizations built on the selected datasets.\n",
    "\n",
    "To add a filter to the dashboard, complete the following steps:\n",
    "\n",
    "-  Return to your dashboard if you've navigated away from it.\n",
    "-  If viewing the published version, switch it to view the draft version of the dashboard.\n",
    "-  Click the <b>Filter</b> icon in the toolbar near the bottom of the canvas.\n",
    "- Place the widget near the top of your dashboard. You may want to add it under your text box. You can rearrange the widgets on the dashboard to organize it the way you want.\n",
    "\n",
    "- When the filter widget is selected, the filter configuration panel appears on the right side of the screen.\n",
    "  \n",
    "-  Apply the following settings:\n",
    "  - <b>Filter</b>: Single value\n",
    "  - <b>Fields</b>: \n",
    "      - sales.total_price \n",
    "      - Count Total Sales.Sales_Goal\n",
    "-  Use the checkboxes to turn on <b>Title</b>.\n",
    "-  Double-click the title on the widget and change it to <b>Product category</b>\n",
    "-  Use the drop-down in the filter widget to test your filter `(e.g., 3000000)`.\n",
    "\n",
    "**Note:** The filter applies to each selected dataset in the filter configuration panel. All of the datasets you selected share the same range of values for product_category. A dashboard viewer can select from that list when choosing which data to filter on the dashboard.\n",
    "\n",
    "You can also use parameters to create interactive dashboards. Parameters allow users to customize visualizations by substituting values into dataset queries at runtime. See <a href=\"https://docs.databricks.com/en/dashboards/parameters.html\" target=\"_blank\">What are dashboard parameters?</a> to learn more as that is beyond the scope of this course.\n",
    "\n",
    "Remember to republish the dashboard after you've made edits to it in order for the published version to reflect your new filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "725b421d-ec93-42b2-9d5b-cf5853450d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The image below is supplied as an example of how your dashboard could appear once you've finished adding visualizations and customizing the colors and features of the dashboard. \n",
    "\n",
    "![Dashboard_Solution](./images/05_Dashboard_Solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9639dcb5-8a7d-4e1a-8f97-02b36de5c13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Presentation with AI/BI Genie Spaces\n",
    "\n",
    "In this lab, you'll be looking into Databricks AI/BI Genie and the data exploration spaces you can create based on both existing dashboards and data sets as well as new combinations of data sets connected to the workspace. \n",
    "\n",
    "This lab uses the following resources from  `dbacademy_retail.v01`:\n",
    "* **sales** table\n",
    "* **customers** table\n",
    "\n",
    "You will also need to have created the following:\n",
    "  * _Your dashboard from previous lab ([05.1 - Creating AI-BI Dashboard in Databricks]($./05.1 - Creating AI-BI Dashboard in Databricks)) (Retail Dashboard)_\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- **Understand Databricks AI/BI Genie Spaces**:\n",
    "   - Explore the concept of Genie Spaces for data exploration and querying.\n",
    "   - Identify the resources and datasets used to create Genie Spaces.\n",
    "\n",
    "- **Create Genie Spaces**:\n",
    "   - Set up a Genie Space from the platform UI using existing datasets.\n",
    "   - Configure Genie Space settings, including table associations, default warehouse, and sample questions.\n",
    "\n",
    "- **Leverage Genie Space Features**:\n",
    "   - Utilize Genie Space functionalities, such as multiple chat threads, data exploration, and monitoring user interactions.\n",
    "   - Edit Genie Space settings, review associated data tables, and monitor user queries.\n",
    "\n",
    "- **Create Genie Spaces from Dashboards**:\n",
    "   - Generate a Genie Space directly from a pre-existing dashboard.\n",
    "   - Interact with the Draft Genie Space and explore how Genie answers questions about the dashboard’s data.\n",
    "\n",
    "- **Understand Genie’s Monitoring and Preview Features**:\n",
    "   - Explore how Genie tracks user questions, ratings, and interactions.\n",
    "   - Recognize the limitations and evolving nature of Genie Spaces in the Public Preview phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd0d36c-413b-400a-b794-5d0eac5d338e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a Genie Space\n",
    "\n",
    "In this part of the lab, we'll start with creating a Genie Space directly from the given UI area. Follow the steps below to create a Genie Space. \n",
    "- Navigate to **Genie** in the left side navigation of the platform. \n",
    "- Click **+ New** in the upper right corner.\n",
    "- A new pop-up will appear prompting you to Connect your data. Within this pop-up, select **All** to locate the table:\n",
    "      - Catalog: dbacademy_retail\n",
    "      - Schema: v01\n",
    "      - Table: customers\n",
    "- When finished selecting data, click **Create** at the bottom.\n",
    "\n",
    "You will now be presented with the Genie Space UI with the chat environment on the left and the settings and details on the left. With the **Configure** button at the top selected, click on **Settings**. (By default, the Configuration opens to **Context.**) Here you can edit the following information:\n",
    "- **Title:** Basic Retail Details\n",
    "- **Description:** \"This Space is designed to provide a space to query the details of the customers dataset.\"\n",
    "- **Default warehouse:** shared_warehouse\n",
    "- **Sample Questions:** \"How many customers do we have in CA?\" (Click the **+ Add** button to add the question.)\n",
    "- Click **Save** at the bottom to confirm the edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe801d5-7c2f-4838-88a4-a04e50fb0989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Genie space's screen area is split into two sides with the chat window on the left and the configuration and settings on the right. The buttons at the top right allow you to choose among these areas:\n",
    "\n",
    "* **+ New chat**: Allows you to create a new threaded dialogues with Genie. After you publish a Genie space to end users (who will probably have only \"Can View\" or \"Can Run\" access), this is one of the only Genie areas they will have access to.\n",
    "* **History** (the icon that looks like a clock): Allows you to review the separate chat threads that you've had with Genie. After you publish a Genie space to end users (who will probably have only \"Can View\" or \"Can Run\" access), this is the other Genie area they will have access to.\n",
    "* **Configure** (the icon that looks like a gear): Returns you to the edit screen for the settings of the Genie space, much like the screen you saw during the space's creation.\n",
    "  Within Configure you'll have:\n",
    "  * **Data** (the icon that looks like a stacked blocks): Allows you to review and edit the data tables associated with the Space.\n",
    "  * **Instructions** (the icon that looks like a book): Allows you to provide general instructions, in natural language, on how Genie will behave when asked a question by a user.\n",
    "  * **SQL Queries** (the icon will look like a command prompt): Allows you to add example SQL queries for Genie to learn from specific to the associated dataset(s).\n",
    "* **Monitoring** (the icon that looks like a eye): Allows you to review what questions were asked, who asked them, and how they were rated by the user. \n",
    "* **Share** (the icon that looks like a lock): Allows you to set the share permissions and share the Genie space with end users. \n",
    "\n",
    "Under the kebab menu, you'll find:\n",
    "* **Benchmarks** (the icon that looks like a graduate's mortarboard): Allows you to define a suite of questions that you run on a recurring basis to ensure the space continues to give good answers to the most important user questions.\n",
    "\n",
    "Additionally, you'll have the options to Clone and delete the Genie space by moving it to trash from this menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51fb8576-9df6-4800-bfdf-6a244768f2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a Genie Space from a Dashboard\n",
    "Alternatively, you can create a Genie Space directly from a dashboard. \n",
    "\n",
    "- Navigate to **Dashboards** and select the dashboard you created during **Lesson 04 - AI/BI Dashboards** (Retail Dashboard).\n",
    "- Switch to the **Draft** view for the dashboard.\n",
    "- Click on **Publish** to open up the publishing dialog box.\n",
    "- From this window, select the toggle for **Genie**. (Note this feature is in Beta currently.)\n",
    "    - You will be given the option to select \"Auto-generate Genie space\" or \"Link existing Genie space.\"\n",
    "    - For this exercise, select \"Auto-generate Genie space\" and click **Publish**.\n",
    "- Navigate to the published version of your Dashboard. \n",
    "- Select the **Ask Genie** option in the upper left corner. This opens a pop-up chat box on top of the dashboard. You can use the kebab menu to access the settings to dock the Genie chat to the side of the screen.\n",
    "- Ask the following question in the chatbox.\n",
    "\n",
    "    > _What tables are there and how are they connected? Give me a short summary._\n",
    "\n",
    "- Review the response provided by Genie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4a8ae9-ccaf-48fc-a179-7967eb2d6bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Data Warehousing Lab\n",
    "\n",
    "This lab will guide you through creating a complete pipeline in Databricks, leveraging Delta Lake, data ingestion techniques, transformations, dashboards, and Databricks Genie. The goal is to give you hands-on experience with the Databricks platform.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Create Delta tables and explore Delta Lake features like Time Travel and Version History.\n",
    "- Perform data ingestion using techniques - Upload UI.\n",
    "- Clean and transform datasets into Bronze, Silver, and Gold layers.\n",
    "- Visualize insights using Databricks Dashboards.\n",
    "- Leverage Databricks Genie for data exploration and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94e68b42-05ec-4fb9-97c2-4a35639754c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating Delta Tables and Exploring Delta Lake Features\n",
    "In this task, you will learn how to create Delta tables and explore the advanced features of Delta Lake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb448806-9471-4e34-9e77-0d337b682e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create the `sales_table` Delta Table\n",
    "Follow these steps to create a Delta table from a CSV file and explore its features:\n",
    "- Create the Delta table by reading data from the CSV file.\n",
    "- Verify the table creation by selecting a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "570e839d-8040-4214-b3f8-b1079102821b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Drop the table if it already exists for lab purposes\n",
    "DROP TABLE IF EXISTS sales_table;\n",
    "\n",
    "---- Create a Delta table using the CSV file\n",
    "CREATE TABLE sales_table USING DELTA\n",
    "AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/databricks_simulated_retail_customer_data/v01/source_files/sales.csv',\n",
    "  <FILL_IN>\n",
    ");\n",
    "\n",
    "---- Select from the newly created table\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1616b1-9aa2-4cd4-b4c0-88bdf2bfbe44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enable Column Mapping and Modify the Table\n",
    "In this step, you will enhance the functionality and structure of the sales_table Delta table by enabling column mapping and modifying the schema. Column mapping is essential for managing schema evolution and ensuring data consistency in Delta Lake. Follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aaccde1-3714-4f05-8762-cf26b57cc2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Enable Column Mapping:**\n",
    "\n",
    "  Set the table properties to enable column mapping. This feature allows you to rename columns, manage schema changes, and maintain backward compatibility for readers.\n",
    "\n",
    "- **Drop Unnecessary Columns:**\n",
    "\n",
    "  Remove the `_rescued_data` column, which is often added to capture extra data during schema inference but may not be required for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2f26a25-4942-4e48-89f6-d1b85b0237f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Enable column mapping on the Delta table\n",
    "ALTER TABLE sales_table SET TBLPROPERTIES (\n",
    "   'delta.minReaderVersion' = <FILL_IN>,\n",
    "   'delta.minWriterVersion' = <FILL_IN>,\n",
    "   'delta.columnMapping.mode' = <FILL_IN>\n",
    ");\n",
    "\n",
    "---- Drop the column after enabling column mapping\n",
    "ALTER TABLE sales_table DROP COLUMNS (<FILL_IN>);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08e7b68a-10b5-4535-95b1-1f3d2cab1941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Add and Update a New Column:**\n",
    "\n",
    "  Add a new column named `discount_code` to the table schema and populate it with values based on conditions. In this step:\n",
    "\n",
    "    - Assign `Discount_20%` to rows where the `product_category` is `'Ramsung'`.\n",
    "    - Assign `N/A` to all other rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7183ab8-aa86-4ba8-a9c4-1a9e4fab0a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Alter the table by adding a new column\n",
    "ALTER TABLE sales_table ADD COLUMNS (discount_code STRING);\n",
    "\n",
    "---- Update the newly added column with data\n",
    "UPDATE sales_table\n",
    "SET discount_code = CASE\n",
    "  WHEN product_category = <FILL_IN> THEN <FILL_IN>\n",
    "  ELSE 'N/A'\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73aa9377-5d94-4e48-94ba-2c7d039626f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **View Table History:**\n",
    "  \n",
    "  Use the `DESCRIBE HISTORY` command to view the version history of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bcf44cb-1c6b-4c06-9002-5921d5fabcc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Display the history of changes made to the sales_table\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4df0574-7b06-463f-a461-1491925b3ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Restore the Table Using Time Travel\n",
    "\n",
    "Delta Lake's time travel feature allows you to access and restore previous versions of a Delta table. This is useful for scenarios such as data recovery, debugging, or auditing changes.\n",
    "\n",
    "In this sub task, you will restore the `sales_table` Delta table to a specific version using the `RESTORE TABLE` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143066e3-6ff5-4b9e-8005-83dd04149fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Restore the sales_table to previous version\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f552232-61cd-4bfa-85ab-1dc3b7063ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Ingestion Techniques\n",
    "In this task, you will learn how to ingest data into Databricks using the UI. This includes downloading a dataset, uploading it to your schema, and creating a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a13c85-78f5-46d4-ab11-7c986051ca72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Uploading Data and Creating a Delta Table using UI\n",
    "\n",
    "- Download the `customers.csv` data file by following [this link](/ajax-api/2.0/fs/files/Volumes/dbacademy_retail/v01/source_files/customers.csv). This will download the CSV file to your browser's download folder.\n",
    "- Using the the [Catalog Explorer](/explore/data/dbacademy) user interface, create a table named *customers_ui* in your schema, using the file you just downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aff68f5-4844-4b84-bb38-ecc76882d40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Verify the Table Creation**\n",
    "\n",
    "  After successfully creating the Delta table, you can verify its creation and view a sample of the data by following these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023f2092-168a-4a81-b864-8d5c99105b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the `SHOW TABLES` command to display all tables in the current schema and confirm that `customers_ui` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c4a972-694b-4f86-b374-a0be32f9db18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Show all tables in the current Schema\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c5feea8-eb74-4734-a556-acdd64f8642b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the `SELECT` statement to retrieve and display the first 10 records from the `customers_ui` table to ensure the data has been ingested correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29e36033-26c2-4472-a6ba-9aa35e9887fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Display the first 10 records from the customers_ui table\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8923ffc1-d41e-402b-a91c-d502fff2e441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Table as Select (CTAS)\n",
    "\n",
    "In this step, we create the `customers_ui_bronze` Delta table by selecting data from `customers_ui` and applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "601fb192-871c-47b6-ad42-99c8d214bd3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Drop the customers_ui_bronze table if it already exists\n",
    "DROP TABLE IF EXISTS customers_ui_bronze;\n",
    "---- Create a new Delta table\n",
    "CREATE TABLE <FILL_IN>\n",
    "SELECT *, \n",
    "  CAST(CAST(valid_from / 1e6 AS TIMESTAMP) AS DATE) AS first_touch_date, \n",
    "  CURRENT_TIMESTAMP() AS updated,\n",
    "  _metadata.file_name AS source_file\n",
    "FROM customers_ui;\n",
    "\n",
    "---- Verify the data in the newly created table\n",
    "<FILL_IN>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c30ddfa-05fa-47ca-8f4e-c8038b40eba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Transformation\n",
    "In this task, you will transform the data in your Delta tables to create the Silver and Gold tables. These transformations will clean, enrich, and join the data to provide valuable insights for analytics and reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0b6430-4a16-4d51-aba0-21a0e50b81a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create the Silver Table\n",
    "\n",
    "The Silver table represents a refined layer with cleaned and enriched data derived from the Bronze table. \n",
    "\n",
    "Follow these steps:\n",
    "- Transform the `customers_ui_bronze` table to clean and enrich the data.\n",
    "- Create a new column, `loyalty_level`, that categorizes customers based on their loyalty segment.\n",
    "- Save the results as the `customers_ui_silver` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14999ddb-64a0-484a-9332-e859ea33a85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Create or replace the Silver table\n",
    "CREATE OR REPLACE TABLE customers_ui_silver AS\n",
    "SELECT \n",
    "  <FILL_IN> \n",
    "  loyalty_segment, ---- Selecting relevant columns from the Bronze table.\n",
    "  CASE \n",
    "    WHEN loyalty_segment = 1 THEN 'High'\n",
    "    WHEN loyalty_segment = 2 THEN 'Medium'\n",
    "    ELSE 'Low'\n",
    "  END AS loyalty_level  ---- Adding a new column, loyalty_level, based on the loyalty_segment values.\n",
    "FROM customers_ui_bronze;\n",
    "\n",
    "---- Verify the Silver table\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7396e1-1771-4031-b468-3cfe1f83e957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create the Gold Table\n",
    "\n",
    "The Gold table represents a business insights layer, created by joining the Silver table with the `sales_table`.\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "- Join the `customers_ui_silver` table with the `sales_table` on the `customer_id` column.\n",
    "- Select key metrics and dimensions required for analytics and save the result as the `customers_ui_gold` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d781603c-b6da-4aa1-88fa-8bd984dafa85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "---- Create or replace the Gold table\n",
    "CREATE OR REPLACE TABLE <FILL_IN> AS\n",
    "SELECT \n",
    "  c.customer_id,\n",
    "  c.customer_name,\n",
    "  <FILL_IN>,\n",
    "  s.product_category,\n",
    "  s.product_name,\n",
    "  s.total_price,\n",
    "  <FILL_IN>\n",
    "FROM customers_ui_silver c\n",
    "JOIN sales_table s ---- Joining the customers_ui_silver table with the sales_table on the customer_id column.\n",
    "ON c.customer_id = s.customer_id; ---- Selecting key attributes from both tables to create a comprehensive insights layer.\n",
    "\n",
    "---- Verify the Gold table\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b2f984-ae3e-4271-9017-08f3d0542361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization with Dashboards\n",
    "In this task, you will create a dashboard in Databricks to visualize insights derived from the Gold table. The task involves adding datasets, creating visualizations, and exploring the dashboard using Databricks Genie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9636c65e-9083-40e2-b7a9-485ac379fa1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a New Dashboard\n",
    "Follow these steps to create a new dashboard:\n",
    "* Navigate to **Dashboards** in the side navigation panel.\n",
    "* Select **Create dashboard**. \n",
    "* At the top of the resulting screen, click on the Dashboard name and change it to **Customer_Sales Dashboard**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af53fb0f-b508-4244-a233-8fd390337277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding Data to the Dashboard\n",
    "\n",
    "To create visualizations, you need to associate datasets with the dashboard. Complete the following steps:\n",
    "\n",
    "- Navigate to the **Data** tab in the dashboard.\n",
    "- Use the **+ Select a table** button to add datasets. \n",
    "- Search for and select the **`customers_ui_gold`** table from *`workspace.default`* and click **Confirm**. The table will appear in your dataset list.\n",
    "\n",
    "You can modify the SQL query associated with each dataset in the query editing panel to customize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e97df2a-1119-4504-b91e-0c48744d0875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualization - Combo Chart\n",
    "Visualize the insights by creating a Combo Chart that displays total sales value and sales order counts over a three-month span.\n",
    "\n",
    "**Steps to Create the Combo Chart:**\n",
    "\n",
    "- In the **Data** tab, select the **+ Create from SQL** option.\n",
    "- Enter and execute the following SQL query:\n",
    "\n",
    "    ```sql\n",
    "    SELECT customer_name, \n",
    "           total_price AS Total_Sales, \n",
    "           date_format(order_date, \"MM\") AS Month, \n",
    "           product_category \n",
    "    FROM workspace.default.customers_ui_gold \n",
    "    WHERE order_date >= to_date('2019-08-01')\n",
    "    AND order_date <= to_date('2019-10-31');\n",
    "    ```\n",
    "\n",
    "- Rename the query to **Three Month Sales** and save it.\n",
    "- Switch to the **Canvas** tab and click **Add a visualization** at the bottom.\n",
    "- Select the **Three Month Sales** dataset and choose the **Combo** chart as the visualization type.\n",
    "- Configure the chart settings:\n",
    "    - **X axis:** Month\n",
    "    - **Bar:** Total_Sales (Rename to **Total Sales Value**)\n",
    "    - **Line:** COUNT(`*`) (Rename to **Count of Sales Orders**)\n",
    "\n",
    "- Enable **dual axis** from the Y-axis configuration menu.\n",
    "- Change the left Y-axis format to **Currency ($)**.\n",
    "\n",
    "This visualization will show the correlation between sales volume and total sales value for each month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814a81d3-3520-415b-abe2-8cf9237fdef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a Genie Space from a Dashboard\n",
    "\n",
    "Databricks Genie allows you to explore data directly from the dashboard in a conversational interface.\n",
    "\n",
    "**Steps to Create a Genie Space:**\n",
    "\n",
    "- Open the **Retail Dashboard** you created.\n",
    "- Switch to the **Draft** view.\n",
    "- Click the kebab menu (three vertical dots) in the upper-right corner and select **Open Draft Genie space**.\n",
    "- In the chatbox, ask:\n",
    "\n",
    "    `\n",
    "    What tables are there and how are they connected? Give me a short summary.\n",
    "    `\n",
    "\n",
    "- Review the response provided by Genie to understand the data relationships and structure.\n",
    "\n",
    "---\n",
    "\n",
    "By completing this task, you have successfully created a visual dashboard to analyze business insights and leveraged Genie for exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "044b2f90-06aa-4092-b345-f657cf4ab766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing the **Data Warehousing Comprehensive Lab**! Throughout this lab, you gained hands-on experience with Databricks to build and analyze a complete data pipeline, leveraging the robust features of Delta Lake, Databricks Dashboards, and Databricks Genie."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6077740724225071,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Get Started with Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
