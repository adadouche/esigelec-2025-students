{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6c24901-28ac-41a6-9552-f07aac4f0152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Apache Spark\n",
    "This beginner-friendly course covers the fundamentals of Apache Spark for large-scale data processing. You will explore Spark’s\n",
    "distributed architecture, master the DataFrame API, and learn to read, write, and process data using Python. Through hands-on\n",
    "exercises, you will build the skills needed to execute Spark transformations and actions efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites: \n",
    "You should meet the following prerequisites before starting this course:\n",
    "\n",
    "- Basic programming knowledge\n",
    "- Familiarity with Python\n",
    "- Basic understanding of SQL queries (`SELECT`, `JOIN`, `GROUP BY`)\n",
    "- Familiarity with data processing concepts\n",
    "- No prior Spark or Databricks experience required\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8639951b-9c13-4c84-8db2-35914c43bb22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Before getting started - Select your notebook compute\n",
    "\n",
    "Before executing cells in this notebook, please select your serverless compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Navigate to the top-right of this notebook and click the *Connect* drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "> **NOTE:** Once you have completed this lab, terminate your cluster:\n",
    ">    Navigate to the top-right of this notebook and click the *Connected* drop-down menu to select your connected cluster. \n",
    ">    When hovering the cluster, options will appear, then select *Terminate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea611cbe-87f5-4c49-b931-89e80d2cc2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 - Exploring Spark Architecture in Databricks\n",
    "\n",
    "In this demonstration, we'll explore how Spark's architecture components manifest in a Databricks environment and how to monitor them using the various UIs available to us.\n",
    "\n",
    "### Objectives\n",
    "- Identify key Spark architecture components in a Databricks cluster\n",
    "- Navigate the Spark UI to monitor application execution\n",
    "- Understand how Databricks implements Spark's cluster management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6cc0aa6-8671-4094-9fe5-42f053fe5880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introduction to Notebooks\n",
    "\n",
    "We will run all of the demos and exercises for this course in notebooks. Notebooks are interactive documents that combine live code, visualizations, narrative text, and outputs in a single document. In Databricks, notebooks provide a powerful environment for data exploration, analysis, and collaboration.\n",
    "\n",
    "Key features of Databricks notebooks:\n",
    "- Code execution: Run code in multiple languages (Python, SQL, R, Scala)\n",
    "- Rich text formatting: Support for Markdown to create well-documented analyses\n",
    "- Cell-based structure: Code and content is organized into executable cells\n",
    "- Interactive visualizations: Direct plotting and charting of results\n",
    "- Collaboration: Share and work together on notebooks in real-time\n",
    "\n",
    "The default language for a notebook is set when you create it, as indicated by the __Python__ selection in the toolbar above☝️. You can change the default language through the notebook settings. Additionally, you can use magic commands to execute code in different languages within the same notebook:\n",
    "\n",
    "- Use `%python` to execute Python code\n",
    "- Use `%sql` to execute SQL queries\n",
    "- Use `%r` to execute R code\n",
    "- Use `%scala` to execute Scala code\n",
    "\n",
    "The `%run` magic command is particularly useful - it allows you to execute another notebook within your current notebook, enabling modular code organization and reuse. For example `%run /path/to/another/notebook`.  \n",
    "\n",
    "This will execute all the cells in the referenced notebook as if they were part of your current notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d62f961-1c10-4dc2-9669-e78818d0a1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The SparkSession and SparkContext\n",
    "\n",
    "The SparkSession is automatically instantiated as `spark` in Databricks notebooks connected to a cluster.  The SparkContext is available via the SparkSession using `spark.sparkContext` or simply `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5792c8b9-3948-4198-83ed-dfe8039dd707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1576ff83-0a96-44b4-918f-003605ff1b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b3d954-295f-4627-8a2f-f33d7f709214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the serverless mode, directly accessing the underlying Spark driver JVM using the attribute 'sparkContext' is not supported on serverless compute. \n",
    "\n",
    "If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ddc776-c2b0-4ccf-99ea-d0a13bc917b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating and Monitoring a Spark Job\n",
    "\n",
    "Let's create a simple job that will help us visualize the execution flow. Run the below cell to create a spark Job\n",
    "\n",
    "**NOTE:** Don't focus too much on the code here. We want to focus on exploring the access logs and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c40917-95ce-4d78-b65c-1d8e33b29657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a large DataFrame to see parallelization in action\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "# Generate some data\n",
    "df = spark.range(0, 1000000)\n",
    "df = df.withColumn(\"square\", col(\"id\") * col(\"id\"))\n",
    "\n",
    "# Force multiple stages with a shuffle operation\n",
    "result = df.groupBy(col(\"id\") % 100).agg(sum(\"square\").alias(\"sum_squares\"))\n",
    "\n",
    "# result = df.repartition(2)\n",
    "\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85d3fd5-177e-4658-8a15-150ab7dff511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Force the computation with the 'count' action\n",
    "print(f\"Number of groups: {result.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37749d4b-f64a-463d-9fd7-198c37d6e708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploring the Query Profile\n",
    "\n",
    "> NOTE: The Spark UI is not available. Instead, use the query profile to view information about your Spark queries. \n",
    "\n",
    "Click on the __See Performance__ link at the bottom of the last cell, then click on the executed statement. A query details panel appears on the right side of the screen.\n",
    "\n",
    "Explore the [Query profile](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aeb0ed7-e4d1-4563-aa53-da86a6a8a41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 - Reading and Writing Data with DataFrames\n",
    "\n",
    "This demonstration will introduce you to Spark DataFrames by reading data from an external source into a DataFrame and then writing the DataFrame out to another location.\n",
    "\n",
    "### Objectives\n",
    "- Read external data into a DataFrame\n",
    "- Create and inspect DataFrame schemas\n",
    "- Write the contents of a DataFrame to a specified location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e422dae0-471d-4209-aaad-8e1c27f642f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading Data \n",
    "\n",
    "Let's start by cchecking your current catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b661d90-24b0-452d-a885-32d49a12cdbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select current_catalog(),current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ab41d76-1094-46f4-8984-68c289cbf8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, let's create a DataFrame by reading a directory of CSV files from the previously created volume using the Market place data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8aebe5-d50d-47f5-b5f9-1c063105cd9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the customers.csv file with schema inference\n",
    "customers_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"/Volumes/databricks_simulated_retail_customer_data/v01/source_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f34e94-b309-409f-8aff-28b1987f3adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the inferred schema for customers_df\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17bf7a09-74ab-4932-bf3f-0446572a87dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> __NOTE:__ If the dataset has a header row, column names are inferred. Without a header, columns are named **_c0**, **_c1**, etc., and must be renamed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fce29d6-6873-4fcf-b9d4-234da3f9e068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternatively we can use below command to print schema in linear format\n",
    "print(customers_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e10bcf38-a722-4b27-93eb-c90029f44a6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `display()` function\n",
    "\n",
    "The `display()` function is available in Databricks runtimes to be used within notebooks.  \n",
    "\n",
    "`display()` represents a Spark action, which returns results for a DataFrame in a formatted tabular result window.  As this is intended for visual inspection only, results are limited to 10,000 rows or 2 MB, whichever is less. Additional features of the `display()` function include:\n",
    "\n",
    "- Ability to sort or filter data in the result set\n",
    "- Ability to download result data to a CSV file\n",
    "- Ability to profile data using the __+__ -> __Data Profile__ option\n",
    "- Ability to visualize data using the __+__ -> __Visualization__ option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b90cd7a-5515-4830-9910-678a765e7b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using Display function to preview the data in the DataFrame\n",
    "display(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5cb9c75-ce7c-4cea-a75b-e5dbc8992901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explicitly Defining a Schema\n",
    "\n",
    "Let's load the same dataset into a DataFrame, this time we will explicitly define the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715c3ebd-4917-4ad5-8aac-674c3dd0f9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "# or ...\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define explicit schema using StructType\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"tax_id\", StringType(), True),\n",
    "    StructField(\"tax_code\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"postcode\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"number\", StringType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"district\", StringType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"ship_to_address\", StringType(), True),\n",
    "    StructField(\"valid_from\", IntegerType(), True),\n",
    "    StructField(\"valid_to\", IntegerType(), True),\n",
    "    StructField(\"units_purchased\", IntegerType(), True),\n",
    "    StructField(\"loyalty_segment\", IntegerType(), True)])\n",
    "\n",
    "# Read the customers.csv file with explicit StructType schema\n",
    "customers_structtype_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .schema(customer_schema) \\\n",
    "  .load(\"/Volumes/databricks_simulated_retail_customer_data/v01/source_files/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2289086a-d999-4f92-ae88-9d795b8de7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examine the explicit schema\n",
    "customers_structtype_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8835b4f2-1195-47be-9eed-4c35b6d284b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using a DDL Schema\n",
    "Let's define the schema now using a DDL schema for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7feffbd2-17ee-4a9d-a0b9-83f21940e1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddl_schema = \"\"\"\n",
    "  customer_id INT NOT NULL,\n",
    "  tax_id STRING,\n",
    "  tax_code STRING,\n",
    "  customer_name STRING,\n",
    "  state STRING,\n",
    "  city STRING,\n",
    "  postcode STRING,\n",
    "  street STRING,\n",
    "  number STRING,\n",
    "  unit STRING,\n",
    "  region STRING,\n",
    "  district STRING,\n",
    "  lon DOUBLE,\n",
    "  lat DOUBLE,\n",
    "  ship_to_address STRING,\n",
    "  valid_from INT,\n",
    "  valid_to INT,\n",
    "  units_purchased INT,\n",
    "  loyalty_segment INT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb1762b-f4c8-4c26-87c9-38705674459a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_ddl_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .schema(ddl_schema) \\\n",
    "  .load(\"/Volumes/databricks_simulated_retail_customer_data/v01/source_files/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3359800d-1fe4-47e1-8d42-1ec508525c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_ddl_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d56f19-ae82-4d1f-9675-508108c2715d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Writing Data From DataFrames\n",
    "\n",
    "Now let's write out the contents of a DataFrame to different output locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e609d78f-da94-4491-97c0-900b68b8c354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing the contents of a DataFrame to a File System\n",
    "We can write the contents of our DataFrame to a filesystem in various formats using the `write` and `save` methods as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc512c3c-24e4-4c4d-b7ba-ba412972d576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define an output path\n",
    "parquet_output_volume_path = f\"/Volumes/workspace/default/v01/customers_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0fd0de-181b-404f-8032-a72da15e573f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fdbc1d-6639-49eb-bbac-e5b2365b48f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame out as Parquet files to a directory\n",
    "customers_ddl_df.write.format(\"parquet\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(parquet_output_volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ebc35c-1e8f-4e5f-bca8-abcbdc6d3982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the files in the directory\n",
    "display(dbutils.fs.ls(parquet_output_volume_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e340de6d-2b38-4314-8399-4e16567cd88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing the contents of a DataFrame to a Table\n",
    "We can also save our DataFrame to a table (defined in a catalog and schema in Unity Catalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d807c0-e27d-4d2a-bb5f-fee44ed241c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing our DataFrame to a new table (this is an action)\n",
    "customers_ddl_df.write.saveAsTable(\"customers_ddl_df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a30a1a3c-375e-4006-864f-83e0da0a98da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternatively you can use the writeTo method which invokes the DataFrameWriterV2 \n",
    "customers_ddl_df.writeTo(                              \n",
    "    f\"customers_ddl_df_table\"\n",
    ").createOrReplace()\n",
    "# you have options to partition the table, as well as append, overwrite or createOrReplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f352c21-8786-44f0-8f4c-5806f553507f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Read back the data in the table\n",
    "SELECT * FROM customers_ddl_df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170c0321-f9e6-4aaa-a180-be8c1efdac3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "1. **Reading Data into DataFrames**:\n",
    "   - Schema inference can be used for CSV files\n",
    "   - The schema can also be explicitly defined (preferred) using StructType definitions or using a DDL schema\n",
    "   \n",
    "2. **Writing Data from DataFrames**:\n",
    "   - DataFrames can be written out to a distributed filesystem (like a Unity Catalog volume)\n",
    "   - DataFrames can also be written out to tables in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56fcc631-5a52-4dc6-ae1f-7c71cb8507b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21243154-18cc-4d8b-95d4-7865a30f176e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 - Flight Data ETL with the DataFrame API\n",
    "\n",
    "This demonstration will walk through common ETL operations using the Flights dataset. We'll cover data loading, cleaning, transformation, and analysis using the DataFrame API.\n",
    "\n",
    "### Objectives\n",
    "- Implement common ETL operations using Spark DataFrames\n",
    "- Handle data cleaning and type conversion\n",
    "- Create derived features through transformations\n",
    "- Use different column reference methods\n",
    "- Work with User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e633795-7431-4952-8d90-09e6e7e081f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Flight Data Processing Requirements\n",
    "\n",
    "#### Source Data\n",
    "Dataset Location: `databricks_airline_performance_data.v01.flights_small`(flight information dataset)\n",
    "\n",
    "#### Target\n",
    "Table name: flight_data\n",
    "\n",
    "Schema:\n",
    "\n",
    "| Column Name | Data Type | Description |\n",
    "|-------------|-----------|-------------|\n",
    "| FlightDateTime | datetime | Datetime of the flight (derived from the Year, Month, DayofMonth, DepTime fields in the source data) |\n",
    "| FlightNum | integer | Flight number |\n",
    "| ElapsedTimeDiff | integer | Difference between scheduled elapsed time and actual elapsed time for the flight, derived from the ActualElapsedTime and CRSElapsedTime fields in the source data |\n",
    "| ArrDelayCategory | string | Categories include \"On Time\", \"Slight Delay\", \"Moderate Delay\" and \"Severe Delay\" based upon the value of the ArrDelay in the source data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e065aeee-d39e-4aef-9b15-0619da411e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Loading and Inspection\n",
    "\n",
    "First, let's load and inspect the flight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd953e1b-7127-40bb-9079-f0a53dc01ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the flights data\n",
    "flights_df = spark.read.table(\"databricks_airline_performance_data.v01.flights_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f90ea3-0561-4ffd-a17f-9e050ce9523c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the schema\n",
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18892b4-4a3e-459f-a9e5-d4a7f73c9ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visually inspect a subset of the data\n",
    "display(flights_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f158020-d9e4-4c63-b8a6-ac8129ef53bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's remove columns we dont need, remember \"filter early, filter often\"\n",
    "flights_required_cols_df = flights_df.select(\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"DayofMonth\",\n",
    "    \"DepTime\",\n",
    "    \"FlightNum\",\n",
    "    \"ActualElapsedTime\",\n",
    "    \"CRSElapsedTime\",\n",
    "    \"ArrDelay\")\n",
    "\n",
    "# Alternatively we could have used the drop() method to remove the columns we didnt want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6325c84c-6253-4863-8b10-f0b410d4112c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a count of the source data records\n",
    "initial_count = flights_required_cols_df.count()\n",
    "\n",
    "print(f\"Source data has {initial_count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0f6514-737b-4428-877c-632f3df0826e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's examine the data for invalid values, these can include nulls or invalid values for string columns \"ArrDelay\", \"ActualElapsedTime\", \"DepTime\" which we intend on performing mathematical operations on, we can use the Spark SQL COUNT_IF function to perform the analysis\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table with cast columns\n",
    "flights_required_cols_df \\\n",
    "    .selectExpr(\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"TRY_CAST(DepTime AS INT) AS DepTime\",\n",
    "        \"FlightNum\",\n",
    "        \"TRY_CAST(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"TRY_CAST(ArrDelay AS INT) AS ArrDelay\"\n",
    "    ) \\\n",
    "    .createOrReplaceTempView(\"flights_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d29bcb6-75f6-4f74-b927-84b54b99e5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Spark SQL to count null values\n",
    "invalid_counts_sql = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT_IF(Year IS NULL) AS Null_Year_Count,\n",
    "    COUNT_IF(Month IS NULL) AS Null_Month_Count,\n",
    "    COUNT_IF(DayofMonth IS NULL) AS Null_DayOfMonth_Count,\n",
    "    COUNT_IF(DepTime IS NULL) AS Null_DepTime_Count,\n",
    "    COUNT_IF(FlightNum IS NULL) AS Null_FlightNum_Count,\n",
    "    COUNT_IF(ActualElapsedTime IS NULL) AS Null_ActualElapsedTime_Count,\n",
    "    COUNT_IF(CRSElapsedTime IS NULL) AS Null_CRSElapsedTime_Count,\n",
    "    COUNT_IF(ArrDelay IS NULL) AS Null_ArrDelay_Count\n",
    "FROM flights_temp\n",
    "\"\"\")\n",
    "\n",
    "display(invalid_counts_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2606c18f-7ff0-4ffd-b800-abd29e62ba36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparing Spark SQL to DataFrame API Operations\n",
    "Spark SQL DataFrame queries and their equivalent operations in the DataFrame API are evaluated to the same physical plans, let's prove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66b09de-5dce-4f93-b66e-65a15863cfbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this is the equivalent of the preceding Spark SQL query using the DataFrame API\n",
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# Make sure to work with the same temporary view that the SQL is using\n",
    "flights_temp_df = spark.table(\"flights_temp\")\n",
    "\n",
    "# Use DataFrame API to count null values\n",
    "invalid_counts_df = flights_temp_df.select(\n",
    "    sum(when(col(\"Year\").isNull(), 1).otherwise(0)).alias(\"Null_Year_Count\"),\n",
    "    sum(when(col(\"Month\").isNull(), 1).otherwise(0)).alias(\"Null_Month_Count\"),\n",
    "    sum(when(col(\"DayofMonth\").isNull(), 1).otherwise(0)).alias(\"Null_DayOfMonth_Count\"),\n",
    "    sum(when(col(\"DepTime\").isNull(), 1).otherwise(0)).alias(\"Null_DepTime_Count\"),\n",
    "    sum(when(col(\"FlightNum\").isNull(), 1).otherwise(0)).alias(\"Null_FlightNum_Count\"),\n",
    "    sum(when(col(\"ActualElapsedTime\").isNull(), 1).otherwise(0)).alias(\"Null_ActualElapsedTime_Count\"),\n",
    "    sum(when(col(\"CRSElapsedTime\").isNull(), 1).otherwise(0)).alias(\"Null_CRSElapsedTime_Count\"),\n",
    "    sum(when(col(\"ArrDelay\").isNull(), 1).otherwise(0)).alias(\"Null_ArrDelay_Count\")\n",
    ")\n",
    "\n",
    "display(invalid_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3736171c-18d9-49cc-b99a-a94ebde2950e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the explain plans for the SQL and DF versions of our query\n",
    "sql_plan = invalid_counts_sql.explain() #Getting SQL Plan Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84154ea6-b950-4d8e-8586-fbb70ce3ae37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_plan = invalid_counts_df.explain() # Getting DF Plan Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb0f469-b9cb-4b19-9d3e-fba77073e579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show that the two approaches evaluate to the same physical plan\n",
    "sql_plan == df_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3724b11e-ae67-4df5-9f23-d61a7642a0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using the Databricks AI Assistant\n",
    "The Databricks AI Assistant feature can be used to generate code or to visualize metrics from DataFrames, from the code cell below click on the __generate__ link and enter:\n",
    "\n",
    "```generate a bar chart showing nulls for each column in the flights_temp_df dataframe```\n",
    "\n",
    "**NOTE:** Click on AI assistance toggle button and Enter the given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06404d93-e369-42b9-b851-16d915a3106f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Add your AI generated code from above cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c18fde74-b86e-427e-8ade-d9fe331464d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The flights data contains some invalid and missing values, lets find them and clean them (in this case we will drop them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d55cb3-4529-4377-abfe-73a0a9c4f65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To drop rows where any specified columns are null, we can use the na.drop DataFrame method\n",
    "non_null_flights_df = flights_required_cols_df.na.drop(\n",
    "    how='any',\n",
    "    subset=['CRSElapsedTime']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbdf8a9-cc78-4659-97d2-456b5537bb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Let's remove rows with invalid values for \"ArrDelay\", \"ActualElapsedTime\" and \"DepTime\" columns\n",
    "flights_with_valid_data_df = non_null_flights_df.filter(\n",
    "    col(\"ArrDelay\").try_cast(\"integer\").isNotNull() & \n",
    "    col(\"ActualElapsedTime\").try_cast(\"integer\").isNotNull() &\n",
    "    col(\"DepTime\").try_cast(\"integer\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b060585-e233-41a4-a2e1-48742d6a30aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now that we know \"ArrDelay\" and \"ActualElapsedTime\" contain integer values only, lets cast them from strings to integers (replacing the existing columns)\n",
    "clean_flights_df = flights_with_valid_data_df \\\n",
    "    .withColumn(\"ArrDelay\", col(\"ArrDelay\").cast(\"integer\")) \\\n",
    "    .withColumn(\"ActualElapsedTime\", col(\"ActualElapsedTime\").cast(\"integer\"))\n",
    "\n",
    "clean_flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f16ea28d-eba7-40ff-b9fb-84cb37e60333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Enrichment\n",
    "\n",
    "Now let's create a useful derived column to categorize delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694c6edb-20f7-4b71-a3b3-44bd5dbe675d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's start by deriving the \"FlightDateTime\" column from the \"Year\", \"Month\", \"DayofMonth\", \"DepTime\" columns, then drop the constituent columns\n",
    "from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substr, lit, pmod\n",
    "\n",
    "flights_with_datetime_df = clean_flights_df.withColumn(\n",
    "    \"FlightDateTime\",\n",
    "    make_timestamp_ntz(\n",
    "        col(\"Year\"),\n",
    "        col(\"Month\"),\n",
    "        col(\"DayofMonth\"),\n",
    "        pmod(substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(1), lit(2)).try_cast(\"integer\"), lit(24)),\n",
    "        substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(3), lit(2)).try_cast(\"integer\"),\n",
    "        lit(0)\n",
    "    )\n",
    ").drop(\"Year\", \"Month\", \"DayofMonth\", \"DepTime\")\n",
    "\n",
    "# Show the result\n",
    "display(flights_with_datetime_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "115c6d9a-9c03-4b91-9a8d-5cfd86496d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets derive the \"ElapsedTimeDiff\" column from the \"ActualElapsedTime\" and \"CRSElapsedTime\" columns\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "flights_with_elapsed_time_diff_df = flights_with_datetime_df.withColumn(\n",
    "    \"ElapsedTimeDiff\", col(\"ActualElapsedTime\") - col(\"CRSElapsedTime\")\n",
    "    ).drop(\"ActualElapsedTime\", \"CRSElapsedTime\")\n",
    "\n",
    "display(flights_with_elapsed_time_diff_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87996f78-9bce-49ba-a71b-72c44155b3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now lets categorize the \"ArrDelay\" column into categories: \"On Time\", \"Slight Delay\", \"Moderate Delay\", \"Severe Delay\"\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "enriched_flights_df = flights_with_elapsed_time_diff_df \\\n",
    "    .withColumn(\"delay_category\", when(col(\"ArrDelay\") <= 0, \"On Time\")\n",
    "        .when(col(\"ArrDelay\") <= 15, \"Slight Delay\")\n",
    "        .when(col(\"ArrDelay\") <= 60, \"Moderate Delay\")\n",
    "        .otherwise(\"Severe Delay\")) \\\n",
    "       .drop(\"ArrDelay\")\n",
    "    \n",
    "display(enriched_flights_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a8cd7d-f150-498a-9fe6-ad00ec6da709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Delays\n",
    "\n",
    "Let's analyze our delay categories using various column referencing approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7fbaa3-a73f-4b56-8391-fe328d803ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Direct reference to list 100 random records\n",
    "display(enriched_flights_df.select(\"FlightNum\", \"delay_category\").limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dda67bd-847c-4d94-aec2-a0b4994dd2cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Column object\n",
    "display(enriched_flights_df.select(col(\"FlightNum\").alias(\"carrier_code\"), col(\"delay_category\")).limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b62c42-8e39-4fef-96b2-42b3db188346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# String expressions\n",
    "display(enriched_flights_df.selectExpr(\"FlightNum\", \"ElapsedTimeDiff\", \"ElapsedTimeDiff > 0 as LongerThanScheduled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9917d13-9c75-4e3b-bfc4-fc9d5ead543c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with UDFs\n",
    "\n",
    "Let's use a vectorized UDF to calculate the z-score (standard deviations from the mean) for delays for each flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a051b4-e380-437a-b589-bfed6fca3b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Pandas UDF (vectorized)\n",
    "@pandas_udf(\"double\")\n",
    "def normalized_diff(diff_series):\n",
    "    return (diff_series - diff_series.mean()) / diff_series.std()\n",
    "\n",
    "# Apply both UDFs\n",
    "udf_example = enriched_flights_df \\\n",
    "    .withColumn(\"diff_normalized\", normalized_diff(\"ElapsedTimeDiff\"))\n",
    "\n",
    "display(udf_example)\n",
    "\n",
    "# Note: In practice, prefer built-in functions over UDFs when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81c9a8e5-2388-4eaa-a836-01d4391cd45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Putting it altogether\n",
    "\n",
    "Let's put this together in a chained operation to manipulate data from a source system and save it to a new target (overwriting any existing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde12898-ea3c-4209-bf39-a23e7fa30989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the target table in case it exists already\n",
    "DROP TABLE IF EXISTS cleaned_and_enriched_flights;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4923061-4425-4621-b595-5ee5b47b3b66",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754658893068}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substr, lit, when, pandas_udf, pmod\n",
    "# or more simply...\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def normalized_diff(diff_series):\n",
    "    return (diff_series - diff_series.mean()) / diff_series.std()\n",
    "\n",
    "a = (spark.read.table(\"databricks_airline_performance_data.v01.flights_small\")\n",
    "    .selectExpr(\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"TRY_CAST(DepTime AS INT) AS DepTime\",\n",
    "        \"FlightNum\",\n",
    "        \"TRY_CAST(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"TRY_CAST(ArrDelay AS INT) AS ArrDelay\"\n",
    "    ).filter(\n",
    "        col(\"ArrDelay\").try_cast(\"integer\").isNotNull() & \n",
    "        col(\"ActualElapsedTime\").try_cast(\"integer\").isNotNull() &\n",
    "        col(\"DepTime\").try_cast(\"integer\").isNotNull()\n",
    "    )\n",
    "    .na.drop()\n",
    "    .withColumn(\n",
    "        \"FlightDateTime\",\n",
    "        make_timestamp_ntz(\n",
    "            col(\"Year\"),\n",
    "            col(\"Month\"),\n",
    "            col(\"DayofMonth\"),\n",
    "            pmod(substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(1), lit(2)).try_cast(\"integer\"), lit(24)),\n",
    "            substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(3), lit(2)).try_cast(\"integer\"),\n",
    "            lit(0)\n",
    "        )\n",
    "    )\n",
    "    .drop(\"Year\", \"Month\", \"DayofMonth\", \"DepTime\")\n",
    "    .withColumn(\n",
    "        \"ElapsedTimeDiff\", col(\"ActualElapsedTime\") - col(\"CRSElapsedTime\")\n",
    "        )\n",
    "    .drop(\"ActualElapsedTime\", \"CRSElapsedTime\")\n",
    "    .withColumn(\"delay_category\", when(col(\"ArrDelay\") <= 0, \"On Time\")\n",
    "        .when(col(\"ArrDelay\") <= 15, \"Slight Delay\")\n",
    "        .when(col(\"ArrDelay\") <= 60, \"Moderate Delay\")\n",
    "        .otherwise(\"Severe Delay\")) \\\n",
    "    .drop(\"ArrDelay\")\n",
    "    .withColumn(\"diff_normalized\", normalized_diff(\"ElapsedTimeDiff\"))\n",
    "    # Write optimized\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"workspace.default.cleaned_and_enriched_flights\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3deff8c0-2c10-46e2-b97e-4db1ed9c3a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM cleaned_and_enriched_flights;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2d510cc-8780-402c-bb88-6e54d251edc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Data Cleaning Best Practices**:\n",
    "   - Validate and clean data types early\n",
    "   - Handle missing values appropriately\n",
    "   - Document cleaning assumptions\n",
    "\n",
    "2. **Data Enrichment**:\n",
    "   - Create meaningful derived columns\n",
    "   - Consider business requirements\n",
    "   - Use functions (built-in or user defined to enrich datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c15e6cc-cb08-4e01-8b90-457c06ac9e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67da22f2-f10f-4d6e-b11b-294d09400a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4 - Lab - Analyzing Transaction Data with DataFrames\n",
    "\n",
    "In this lab, you'll analyze transactions from the Bakehouse dataset using Spark DataFrames. You'll apply the concepts from the lecture to solve real business problems and gain insights from the data.\n",
    "\n",
    "### Objectives\n",
    "- Reading data into a DataFrame and exploring its contents and structure\n",
    "- Filtering records and projecting columns from a DataFrame\n",
    "- Saving a DataFrame to a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bfb125f-6ed3-4885-9d00-09642be47ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Setup and Data Loading\n",
    "\n",
    "First, let's load our data and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74377e2-ea4e-40da-be78-f91822380c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read the Bakehouse transaction data\n",
    "transactions_df = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "\n",
    "## Examine the schema and display first few rows\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc673e37-c99f-47a5-b75f-b88dc13c7889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the basic characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1272b1-0495-4a89-9419-c513a9e510da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Total Transactions Count\n",
    "Get a count of all transactions helps us understand the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "927d1f0b-1e45-4cbd-8e81-42e636de9a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FILL_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0b9675-538d-440c-afd6-2321b3806892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transactions over $100\n",
    "Find the transactions over $100, save these to a new DataFrame named `large_transactions_df`.  Display the contents of this new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e607d22a-bf4b-495a-80f0-3b81d30ad7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "FILL_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5954ea90-ec41-4d72-96e1-b4573f6380ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save the DataFrame to a table\n",
    "Save the `large_transactions_df` DataFrame to a table called `large_transactions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acea0fe1-2f02-4348-9a06-47ba68acacae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FILL_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e80fec6-4c57-4df3-8217-4520a75ea726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use Spark SQL to count the number of large transactions\n",
    "Count the total number of large transactions in our `large_transactions` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0748f817-6339-4f35-b1e9-b45e1018695f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FILL_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e40eebd-7617-47d2-a23c-7b8027464f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cleanup\n",
    "\n",
    "\n",
    "> **NOTE:** Once you have completed this lab, terminate your cluster:\n",
    ">    Navigate to the top-right of this notebook and click the *Connected* drop-down menu to select your connected cluster. \n",
    ">    When hovering the cluster, options will appear, then select *Terminate*."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6842966073058014,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Introduction to Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
