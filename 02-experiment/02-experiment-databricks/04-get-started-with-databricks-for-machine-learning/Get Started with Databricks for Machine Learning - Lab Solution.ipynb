{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c207f3-19e8-41b8-b7c0-3496705015e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Getting Started with Databricks for Machine Learning\n",
    "\n",
    "In this lab, we will construct a comprehensive ML model pipeline using Databricks. Initially, we will train and monitor our model using mlflow. Subsequently, we will register the model and advance it to the next stage. In the latter part of the lab, we will utilize Model Serving to deploy the registered model. Following deployment, we will interact with the model via a REST endpoint and examine its behavior through an integrated monitoring dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef7aa02-190d-44e8-9473-9837d4e069ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac73f34-ba0b-4fa3-a70e-e6724a1ebf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U databricks-feature-engineering -qqq --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a95d0dca-3736-4c05-8147-4601a1622b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Â Imports and default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6af8c49-bf16-4053-bd3a-91cefc056502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "current_username = spark.sql(\"SELECT current_user()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11241cde-06bf-4ce1-83fd-727b76361905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Ingestion\n",
    " - The first step in this lab is to ingest data from .csv files and save them as delta tables. Navigate to the Catalog explorer and locate the datasets under shared and find `dbacademy_airbnb`. Expand `v01` and `locate airbnb-cleaned-mlflow.csv` located in the volume `sf-listings`. \n",
    " - Second, we grab a few relevant features to help train our model to predict the target variable for this dataset, `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47505f50-0ddd-4a2e-924e-d4e97c2a40cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/Volumes/databricks_airbnb_sample_data/v01/sf-listings/airbnb-cleaned-mlflow.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afdaba9-fc7e-4fa7-938c-e9d2ac073c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_table = \"airbnb_lab\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0dab57b-b1f2-4791-8aa6-67fe11ac0283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's preprocess this dataset since the schema shows all variables being of time `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727b9526-fe65-4d14-8d12-5ffd0716b773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
    "\n",
    "## Specify columns that should be treated as categorical (e.g., integers in categorical context)\n",
    "categorical_columns = ['neighbourhood_cleansed', 'zipcode', 'property_type', 'room_type', 'bed_type']\n",
    "for col in categorical_columns:\n",
    "    df = df.withColumn(col, df[col].cast(StringType()))\n",
    "\n",
    "## Specify columns that should remain as floats for machine learning\n",
    "numerical_columns = ['host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', \n",
    "                 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "                 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'price']\n",
    "for col in numerical_columns:\n",
    "    df = df.withColumn(col, df[col].cast(FloatType()))\n",
    "\n",
    "df = df.withColumn(\"airbnb_id\", F.monotonically_increasing_id()).select(['airbnb_id'] + numerical_columns + categorical_columns)\n",
    "\n",
    "## Check the schema to confirm data type changes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f327fcc-4896-4b73-8177-1e471bdc899a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('delta').mode('overwrite').saveAsTable('airbnb_lab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5931fb63-2217-453e-a95e-7204a2deedcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8551c83-e1a2-4be2-985d-7dde3311be6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, using PySpark, create a DataFrame called `feature_df` that is the feature table. Recall that the feature table must contain a primary key and does not contain the target variable, which is `price` in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0599ef2d-3574-407b-86a5-49f6c119dc6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_df = df.select(['airbnb_id'] + numerical_columns)\n",
    "\n",
    "## Find rooms with a score of at least 6.0 and 80 reviews\n",
    "feature_df = feature_df.filter((df.review_scores_rating >= 6.0) & (df.number_of_reviews >= 80))\n",
    "display(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03644294-435f-4468-b3e0-e559e5c739ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write to Databricks Feature Store. Remember, we do not include our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc91717d-bfd6-4968-a833-f9bbe68fc213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Write feature_df to Databricks Feature Store.\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "feature_df = feature_df.drop(\"price\")\n",
    "\n",
    "fs.create_table(\n",
    "    name=\"airbnb_features\",\n",
    "    primary_keys = ['airbnb_id'], \n",
    "    df = feature_df,\n",
    "    description = \"This is the airbnb feature table\",\n",
    "    tags = {\"source\": \"bronze\", \"format\": \"delta\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "073932e7-4a09-481f-8a82-0c7b01b31605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train a Model\n",
    "To summarize what you have accomplished so far:\n",
    "1. You have created a table that is a snapshot of the original dataset (Airbnb csv file) called `airbnb_lab`.\n",
    "1. You have created a feature table and stored it in Databricks Feature Store called `airbnb_features`.\n",
    "\n",
    "Next, we will simulate the process of reading in these Delta tables and training a model. We will train a machine learning model and register it to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf24d16-06fe-4da0-b830-0564c48694e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read in the feature table airbnb_features from Unity Catalog using PySpark and store it as training_df\n",
    "prediction_df = spark.read.format('delta').table('airbnb_lab').select('airbnb_id','price')\n",
    "features_df = spark.read.format('delta').table('airbnb_features')\n",
    "\n",
    "## Join these two dataframes on airbnb_id\n",
    "training_df = prediction_df.join(features_df, on='airbnb_id').drop('airbnb_id')\n",
    "training_pdf = training_df.toPandas()\n",
    "\n",
    "## Perform train-test split\n",
    "X = training_pdf.drop(columns = ['price'])\n",
    "y = training_pdf['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69deeb1d-8302-42bc-8180-5cda20cefd45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set the path for mlflow experiment\n",
    "mlflow.set_experiment(f\"/Users/{current_username}/model-serving-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903cd563-1b1f-415d-9c98-8be93315eeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Start the MLflow run\n",
    "with mlflow.start_run(run_name=\"model-serving-run\") as run:\n",
    "    ## Initialize the Random Forest classifier\n",
    "    rf_classifier = sklearn.ensemble.RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    ## Fit the model on the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    ## Make predictions on the test data\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    ## Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(log_input_examples=True, silent=True)\n",
    "    ## Calculate F1 score with 'macro' averaging for multiclass\n",
    "    mlflow.log_metric(\"test_f1\", sklearn.metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    ## mlflow.log_metric(\"test_f1\", f1_score(y_test, y_pred))\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        rf_classifier,\n",
    "        artifact_path=\"model-artifacts\",\n",
    "        input_example=X_train[:3],\n",
    "        signature=infer_signature(X_train, y_train),\n",
    "    )\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model-artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa1777c-c460-452c-a1ff-901ed706164f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, we explored the full potential of Databricks Data Intelligence Platform for machine learning tasks. From data ingestion to model deployment, we covered essential steps such as data preparation, model training, tracking, registration, and serving. By utilizing MLflow for model tracking and management, and Model Serving for deployment, we demonstrated how Databricks offers a seamless Lakeflow Jobs for building and deploying ML models. Through this comprehensive lab, users can gain a solid understanding of Databricks capabilities for ML tasks and streamline their development process effectively."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get Started with Databricks for Machine Learning - Lab Solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
