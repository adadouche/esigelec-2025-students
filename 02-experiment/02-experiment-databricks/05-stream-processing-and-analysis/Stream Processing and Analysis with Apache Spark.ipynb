{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef3f095-f454-4bc3-ac30-ab2553ce1d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stream Processing and Analysis with Apache Spark\n",
    "\n",
    "Learn the essentials of stream processing and analysis with Apache Spark in this course. Gain a solid understanding of stream\n",
    "processing fundamentals and develop applications using the Spark Structured Streaming API. \n",
    "\n",
    "Explore advanced techniques such as\n",
    "stream aggregation and window analysis to process real-time data efficiently. This course equips you with the skills to create scalable\n",
    "and fault-tolerant streaming applications for dynamic data environments.\n",
    "\n",
    "---\n",
    "\n",
    "###Prerequisites: \n",
    "You should meet the following prerequisites before starting this course:\n",
    "\n",
    "- Basic programming knowledge\n",
    "- Familiarity with Python\n",
    "- Basic understanding of SQL queries (`SELECT`, `JOIN`, `GROUP BY`)\n",
    "- Familiarity with data processing concepts\n",
    "- Developing Application with Spark or Prior Databricks Experience is required\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63a5958-543c-44dd-8161-cdee513b4978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e9fb51-0b9f-41ed-9331-ff1bb8a430c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "current_username = spark.sql(\"SELECT current_user()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cad79cd-36c9-4e2e-96d7-61be54a5bc32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Default values\n",
    "\n",
    "In order to use Structured Streaming, you will need to store checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f80643-2df7-4eb9-873c-28f4b4d48297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_location_prefix= '/Volumes/workspace/default/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ac0aa4e-a51e-48d1-8bac-61634bdadf63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP VOLUME IF EXISTS checkpoint;\n",
    "CREATE VOLUME IF NOT EXISTS checkpoint;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f9151f-2820-45ef-b527-e334e9bd048f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Spark Structured Streaming\n",
    "\n",
    "This notebook demonstrates key concepts of Structured Streaming using practical examples with IoT sensor data.\n",
    "\n",
    "### Objectives\n",
    "- Understand stream processing fundamentals\n",
    "- Work with different streaming sources and sinks\n",
    "- Implement streaming transformations\n",
    "- Use watermarking and windowing\n",
    "- Monitor streaming queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03a68d5c-89aa-41d9-92da-96dad23243dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stream Processing Setup\n",
    "\n",
    "First, let's set up our streaming infrastructure and define our schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78ed4ef-b637-461b-b0a9-d212ee4a59d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries if not already imported\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema for the stream\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", LongType(), True),\n",
    "    StructField(\"notifications\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"order_timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Now use this schema for your streaming DataFrame\n",
    "stream_df = (spark.readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .option(\"path\", \"/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/orders/stream_json\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d746eb3e-edad-4e7e-995a-eab37fa5eb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confirm we have set up a streaming dataframe\n",
    "print(f\"isStreaming: {stream_df.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a78cb6-937d-4ce8-b9ac-d1c8a1b22aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Running Basic Streaming Queries\n",
    "\n",
    "Now let's kick off some streaming queries.\n",
    "\n",
    "But first you need to create a volume to host the check point files needed to support the streaming api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92329b9-eddf-4fa3-ba1d-72665cf81ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the stream for testing, display will start an implicit query (this is analogous to the console sink)\n",
    "display(stream_df, checkpointLocation = f'{checkpoint_location_prefix}/stream_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9069c5c3-d92a-408d-8a42-1d08d27d22a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Transformations on the Stream\n",
    "Stateless streaming transformations are analogous to narrow transformations we would perform on normal DataFrames (`select`, `filter`, `withColumn`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6485de7-60db-4a46-aa51-74e310989020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple transformations using standard DataFrame operations\n",
    "transformed_stream = stream_df \\\n",
    "    .withColumn(\"notification_status\", col(\"notifications\").isNotNull()) \\\n",
    "    .withColumn(\"order_details\", concat(lit(\"Order #\"), col(\"order_id\").cast(\"string\")))\n",
    "\n",
    "# Display the transformed stream\n",
    "display(transformed_stream, checkpointLocation = '/Volumes/workspace/default/checkpoint/transformed_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51adecc-1abc-4c76-b0a5-529702c40125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for only orders with notifications enabled\n",
    "notifications_stream = stream_df \\\n",
    "    .filter(col(\"notifications\") == \"Y\")\n",
    "\n",
    "# Display filtered stream\n",
    "display(notifications_stream, checkpointLocation='/Volumes/workspace/default/checkpoint/notifications_stream', outputMode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0856f4-0dd0-4adf-9534-02eb922e2ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop any existing queries with the same name\n",
    "for q in spark.streams.active:\n",
    "    if q.name == \"orders_streaming_table\":\n",
    "        q.stop()\n",
    "\n",
    "# Write to memory sink for interactive querying\n",
    "memory_query = stream_df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/workspace/default/checkpoint/memory_query\") \\\n",
    "    .queryName(\"orders_streaming_table\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f594ef53-f40d-4fd4-bb94-5304d876f4fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Now you can query the in-memory table using SQL\n",
    "SELECT notifications, count(*) as num_notifications \n",
    "FROM orders_streaming_table GROUP BY notifications\n",
    "\n",
    "-- try running the query multiple time to see the count increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b02c1a-d4fd-417f-aeca-aeddf8caf121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combining Multiple Streams using Union\n",
    "\n",
    "Different stream sources can be combined using relational operators like `union`.  Let's have a look.\n",
    "\n",
    "> **NOTE:** Stream-to-static DataFrame joins are fully supported and easy to implement (for example joining a stream with reference or static lookup data for enrichment). Stream-to-stream joins are supported as well, but require special handling for state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d58ccd-ae22-4cb1-bd35-a40b557fd61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a second stream with a subset of the data\n",
    "filtered_stream1 = stream_df.filter(col(\"notifications\") == \"Y\")\n",
    "filtered_stream2 = stream_df.filter(col(\"notifications\") == \"N\")\n",
    "\n",
    "# Union the streams\n",
    "combined_stream = filtered_stream1.union(filtered_stream2)\n",
    "\n",
    "# Process the combined stream\n",
    "display(combined_stream, checkpointLocation = '/Volumes/workspace/default/checkpoint/combined_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f78f83-293a-4945-986f-9235924dbe20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Triggers to Control Processing\n",
    "\n",
    "Triggers control how long a batch window is, let's show an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008f8cfb-1015-4e82-98b2-08edd5b427ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop any existing queries with the same name\n",
    "for q in spark.streams.active:\n",
    "    if q.name == \"triggered_query_table\":\n",
    "        q.stop()\n",
    "\n",
    "# Process data in micro-batches every 10 seconds\n",
    "triggered_query = stream_df \\\n",
    "    .withColumn(\"processing_ts\", current_timestamp()) \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"triggered_query_table\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/workspace/default/checkpoint/triggered_query\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502d74e9-a981-4fb5-a98f-1a4897831f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT processing_ts, count(*) as count \n",
    "FROM triggered_query_table \n",
    "GROUP BY processing_ts\n",
    "ORDER BY processing_ts\n",
    "\n",
    "-- run the query multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531a3ba9-aae2-41dd-92e1-80ffdc8d6d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Stream Processing Fundamentals**:\n",
    "   - Continuous data processing\n",
    "   - Schema definition\n",
    "   - Basic transformations\n",
    "\n",
    "2. **Sources and Sinks**:\n",
    "   - Rate source for testing\n",
    "   - Console sink for debugging\n",
    "   - Memory sink for monitoring\n",
    "\n",
    "4. **Monitoring and Management**:\n",
    "   - Query monitoring\n",
    "   - Progress tracking\n",
    "   - Resource management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3784d1e-4515-491d-94f7-c0d8ba4e998f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to stop the active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24213da5-779b-4e41-b309-79e88234aeea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11def38-6f31-403a-bea2-102aedb603a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Introduction to Spark Structured Streaming\n",
    "\n",
    "In this lab, you'll work with a streaming dataset containing order status updates. You'll learn how to create streaming DataFrames, perform basic transformations, and work with different streaming sinks.\n",
    "\n",
    "### Objectives\n",
    "- Understand stream processing fundamentals\n",
    "- Implement basic streaming operations\n",
    "- Work with different streaming sources and sinks\n",
    "- Apply streaming transformations and watermarking\n",
    "- Handle late data and monitor streaming queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d24076d-5f53-40d9-9d3f-5bee387e5b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stream Processing Setup\n",
    "\n",
    "First, let's set up our streaming infrastructure and define our data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b50d2d-5ad6-46e9-a140-1a94b4ee5d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# --# 1. Create a schema for the status updates with these fields:\n",
    "# --#  - order_id (LongType)\n",
    "# --# - order_status (StringType)\n",
    "# --# - status_timestamp (LongType)\n",
    "# --# 2. Create a streaming DataFrame that reads JSON files from the path:\n",
    "# --#   /Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/status/stream_json\n",
    "# --# 3. Set maxFilesPerTrigger to 1\n",
    "# --# 4. Verify that you have created a streaming DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe429cc-b159-4920-942e-67f394a9135a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streaming Queries\n",
    "\n",
    "Now we will create a basic streaming query, using the memory sink which we will subsequently query using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d4d0e5-6ad9-40b7-a385-ca7e2772178c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the results of your status_stream into a memory sink with a query name of \"order_status_streaming_table\", appending records to the output sink\n",
    "\n",
    "# Stop any existing queries with the same name\n",
    "for q in spark.streams.active:\n",
    "    if q.name == \"order_status_streaming_table\":\n",
    "        q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31f7d8e-9955-414d-b7f4-2a38bb73ce68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the number of the different types of order_status values in the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5357d7e6-5daa-4372-9fc3-778db6686777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Transformations\n",
    "\n",
    "Now, you'll perform some basic transformations on the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8603203c-9085-499f-81ed-49213261c3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --# 1. Create a new streaming DataFrame that:\n",
    "# --#   - Converts the status_timestamp to a timestamp type column named \"event_time\"\n",
    "# --#   - Creates a new column \"status_description\" that adds a descriptive prefix to the status value\n",
    "# --#   - Creates a new column \"is_completed\" that is TRUE when order_status is \"delivered\" or \"canceled\", and FALSE otherwise\n",
    "# --# 2. Display the transformed stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b203f36f-e4da-449a-abaa-fca5f26b30b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Controlling Processing with Triggers\n",
    "\n",
    "Finally, you'll use triggers to control how the stream processes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b7eb6f0-3290-4c72-ac67-4987721ae302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --# 1. Create a triggered streaming query that:\n",
    "# --#   - Processes data from the status_stream every 15 seconds\n",
    "# --#   - Adds a processing_time timestamp column\n",
    "# --#   - Writes to a memory sink named \"triggered_status_updates\"\n",
    "# --# 2. Run a SQL query to see the processing batches\n",
    "\n",
    "# --# Stop any existing queries with the same name\n",
    "for q in spark.streams.active:\n",
    "    if q.name == \"triggered_status_updates\":\n",
    "        q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c111054c-cdfe-49b7-b904-6c0bff882589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the processing batches in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6d4c46-44a5-49e8-b4ec-508643ea1bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Stream Processing Fundamentals**\n",
    "   - Structured Streaming provides a DataFrame-based streaming API\n",
    "   - Supports both batch and streaming processing models\n",
    "   - Handles data consistency and fault tolerance\n",
    "\n",
    "2. **Sources and Sinks**\n",
    "   - Multiple input sources available (Rate, File, Kafka, etc.)\n",
    "   - Various output sinks for different use cases\n",
    "   - Memory sink useful for testing and debugging\n",
    "\n",
    "3. **Data Processing**\n",
    "   - Supports standard DataFrame operations\n",
    "   - Windowing and watermarking for time-based processing\n",
    "   - Aggregations and streaming joins\n",
    "\n",
    "4. **Monitoring and Management**\n",
    "   - Built-in query monitoring capabilities\n",
    "   - Progress tracking and metrics\n",
    "   - Late data handling strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24f0e9ed-ad06-4bec-9158-e180dac404c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to stop the active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63df08a7-8344-44b6-96a5-36d0b291f2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f44316d5-66fd-4e33-8b26-eaaa11cf3a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Window Aggregation in Spark Structured Streaming\n",
    "\n",
    "This notebook demonstrates advanced concepts of Structured Streaming including stateful operations, state management, streaming joins, and window operations.\n",
    "\n",
    "### Objectives\n",
    "- Understand stateful vs stateless operations\n",
    "- Implement windowed operations\n",
    "- Perform streaming joins\n",
    "- Work with late arriving data using watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "871f102b-b4a6-4c97-a1e7-942ef192cae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Data Sources\n",
    "\n",
    "First, let's create two streaming DataFrames that we'll use throughout our demo:\n",
    "1. An **orders stream** containing customer orders\n",
    "2. A **status stream** containing order status updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4044ae32-b687-4c07-9935-1fe12f36ca3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define schema for orders\n",
    "orders_schema = StructType([\n",
    "    StructField(\"customer_id\", LongType(), True),\n",
    "    StructField(\"notifications\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"order_timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for status updates\n",
    "status_schema = StructType([\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"status_timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Create orders streaming DataFrame\n",
    "orders_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(orders_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"path\", \"/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/orders/stream_json\") \\\n",
    "    .load()\n",
    "\n",
    "# Create status streaming DataFrame\n",
    "status_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(status_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"path\", \"/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/status/stream_json\") \\\n",
    "    .load()\n",
    "\n",
    "# Verify both are streaming DataFrames\n",
    "print(f\"orders_stream is streaming: {orders_stream.isStreaming}\")\n",
    "print(f\"status_stream is streaming: {status_stream.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b3cdfc-23eb-4824-bd15-9fcc2f9d80ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stateless vs Stateful Operations\n",
    "\n",
    "Let's look at the difference between stateless and stateful operations:\n",
    "\n",
    "- **Stateless operations**: Process each record independently (e.g., `select`, `filter`)\n",
    "- **Stateful operations**: Maintain information across batches (e.g., `groupBy`, `join`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10ad8d83-55a0-401a-8577-b7c4d21fdd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stateless Operation Example\n",
    "Let's apply some simple stateless transformations to our streams:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e81406-fe44-4651-8849-f9b3bffa997f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(orders_stream, checkpointLocation = f'{checkpoint_location_prefix}/orders_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82cab873-ccda-41da-9df8-926090964223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(status_stream, checkpointLocation = f'{checkpoint_location_prefix}/status_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a022b409-c2ec-485e-b959-3119aca9a182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert timestamps to a more usable format (stateless operation)\n",
    "orders_transformed = orders_stream \\\n",
    "    .withColumn(\"order_time\", from_unixtime(col(\"order_timestamp\")).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"notification_enabled\", col(\"notifications\") == \"Y\")\n",
    "\n",
    "# Display the transformed stream\n",
    "display(orders_transformed, checkpointLocation = f'{checkpoint_location_prefix}/orders_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0086ad5-aeff-4a59-91a0-9aaa00b8efa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stateful Operation Example\n",
    "Now let's perform some stateful operations that maintain state across batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96db492c-47c5-4adf-9b58-2e4fb0a3d4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count orders by status (stateful aggregation)\n",
    "status_counts = status_stream \\\n",
    "    .groupBy(\"order_status\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "display(status_counts, checkpointLocation = f'{checkpoint_location_prefix}/status_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1cbab7a-ac08-4a96-a3f6-df94845ae9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Window Operations\n",
    "\n",
    "Window operations allow us to perform aggregations over time windows. We'll demonstrate:\n",
    "- Tumbling Windows (fixed, non-overlapping)\n",
    "- Sliding Windows (overlapping windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f2c76cf-ac4f-4d04-8763-da195814a173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tumbling Window Example\n",
    "\n",
    "Let's count orders per 1-minute tumbling window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229dcc32-40c0-4318-810d-e4f8f9560c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, make sure to clean up previous streams with same name\n",
    "for query in spark.streams.active:\n",
    "    if query.name == \"tumbling_window_counts\":\n",
    "        query.stop()\n",
    "\n",
    "# Prepare data by ensuring we have a proper timestamp column\n",
    "status_events = status_stream \\\n",
    "    .withColumn(\"event_time\", from_unixtime(col(\"status_timestamp\")).cast(\"timestamp\"))\n",
    "\n",
    "# Group by status and 1-minute tumbling windows\n",
    "tumbling_windows = status_events \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        col(\"order_status\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Write to memory for visualization\n",
    "tumbling_window_query = (tumbling_windows.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'{checkpoint_location_prefix}/tumbling_window_query')\n",
    "    .queryName(\"tumbling_window_counts\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e29ceb-8fd8-4668-81d0-13509dea83b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query the tumbling window results\n",
    "SELECT \n",
    "  window.start as window_start,\n",
    "  window.end as window_end,\n",
    "  order_status,\n",
    "  count\n",
    "FROM tumbling_window_counts\n",
    "ORDER BY window_start, order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170acd08-16fa-4b43-9130-018622ccb7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sliding Window Example\n",
    "Now let's count orders per 2-minute window, sliding every 1 minute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec70d7d-3951-4c19-bc7a-db2c8032196d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop any existing queries with the same name\n",
    "for query in spark.streams.active:\n",
    "    if query.name == \"sliding_window_counts\":\n",
    "        query.stop()\n",
    "\n",
    "# Group by status and sliding window\n",
    "sliding_windows = status_events \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"2 minutes\", \"1 minute\"),\n",
    "        col(\"order_status\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Write to memory for visualization\n",
    "sliding_window_query = (sliding_windows.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'{checkpoint_location_prefix}/sliding_window_query') \n",
    "    .queryName(\"sliding_window_counts\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d73b5a7-9bcf-40b9-a7ab-7ae5498a9e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query the sliding window results\n",
    "SELECT \n",
    "  window.start as window_start,\n",
    "  window.end as window_end,\n",
    "  order_status,\n",
    "  count\n",
    "FROM sliding_window_counts\n",
    "ORDER BY window_start, order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f42708e7-8e56-40ee-9aab-9a38869bdade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streaming Joins\n",
    "\n",
    "Let's demonstrate joining our streaming order data with status updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce7dadf-98b7-42a7-9742-2fe626755ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare our streaming DataFrames with proper timestamps\n",
    "orders_with_time = orders_stream \\\n",
    "    .withColumn(\"order_time\", from_unixtime(col(\"order_timestamp\")).cast(\"timestamp\"))\n",
    "\n",
    "status_with_time = status_stream \\\n",
    "    .withColumn(\"status_time\", from_unixtime(col(\"status_timestamp\")).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5295679-2229-4737-bc25-72c689128cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream-Static Join\n",
    "First, let's create a static DataFrame for lookup purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db535fdc-46d4-42e5-970b-1bba72ccfc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a static lookup table for order status descriptions\n",
    "status_lookup = spark.createDataFrame([\n",
    "    (\"placed\", \"Order has been placed\"),\n",
    "    (\"preparing\", \"Order is being prepared\"),\n",
    "    (\"on the way\", \"Order is in transit\"),\n",
    "    (\"delivered\", \"Order has been delivered\"),\n",
    "    (\"cancelled\", \"Order has been cancelled\")\n",
    "], [\"order_status\", \"status_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f32c1e-3dba-47d9-922b-d841b75bd398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join streaming status data with static status descriptions\n",
    "enriched_status = status_with_time \\\n",
    "    .join(status_lookup, \"order_status\")\n",
    "\n",
    "# Display the joined stream\n",
    "display(enriched_status, checkpointLocation = f'{checkpoint_location_prefix}/enriched_status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15075b0-c168-4185-a184-f8eb5698c539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream-Stream Join\n",
    "Now let's join our two streaming DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7246ee94-c5f1-4993-8685-319b27931401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop any existing queries\n",
    "for query in spark.streams.active:\n",
    "    if query.name == \"order_status_join\":\n",
    "        query.stop()\n",
    "\n",
    "# Join order stream with status stream on order_id\n",
    "# Note: We need to limit state buildup for production use\n",
    "order_status_join = orders_with_time \\\n",
    "    .join(\n",
    "        status_with_time,\n",
    "        \"order_id\"\n",
    "    )\n",
    "\n",
    "# Write to memory sink\n",
    "order_status_join_query = (order_status_join.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'{checkpoint_location_prefix}/order_status_join')\n",
    "    .queryName(\"order_status_join\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c5df20-0218-4136-8f7f-7c77e15d4d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query the joined data\n",
    "SELECT \n",
    "  order_id, \n",
    "  customer_id, \n",
    "  order_status,\n",
    "  notifications,\n",
    "  order_time,\n",
    "  status_time\n",
    "FROM order_status_join\n",
    "LIMIT 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67c0bc87-900a-4e4a-aef8-63ecec468ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Late Data with Watermarks\n",
    "Watermarks help us handle late-arriving data by defining how long to wait for late events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a53125-c320-4c7b-83c0-b826c22b42b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop any existing queries\n",
    "for query in spark.streams.active:\n",
    "    if query.name == \"windowed_with_watermark\":\n",
    "        query.stop()\n",
    "\n",
    "# Add watermark to status events\n",
    "status_with_watermark = status_events \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")\n",
    "\n",
    "# Windows with watermark\n",
    "watermarked_windows = status_with_watermark \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),\n",
    "        col(\"order_status\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Write to memory\n",
    "query5 = (watermarked_windows.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'{checkpoint_location_prefix}/watermarked_windows')\n",
    "    .queryName(\"windowed_with_watermark\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cfb0e5-c0cf-4591-8cbb-2269a6bf84ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query the windowed data with watermark\n",
    "SELECT \n",
    "  window.start as window_start,\n",
    "  window.end as window_end,\n",
    "  order_status,\n",
    "  count\n",
    "FROM windowed_with_watermark\n",
    "ORDER BY window_start, order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33d1b49-0db2-4f2e-843e-cd0a5cdb5b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to stop the active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9f2a88-aa22-40a6-a599-c60bec075300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c2111e-75f2-48b7-a4c4-edca588784a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Window Aggregation in Spark Structured Streaming\n",
    "\n",
    "In this lab, you'll work with stateful operations, sliding windows, and watermarks in Spark Structured Streaming. You'll analyze streams of order and status data to derive meaningful insights.\n",
    "\n",
    "### Objectives\n",
    "- Implement stateful aggregations and window operations\n",
    "- Handle late data and state management\n",
    "- Build real-time monitoring systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f19ae70-9140-4585-b1fe-6291dd0d6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Data Sources\n",
    "\n",
    "First, let's set up our streaming environment with the necessary data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a5d7f6-4e8b-4988-a6dd-a8d23eb1aaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Schemas are provided for you\n",
    "orders_schema = StructType([\n",
    "    StructField(\"customer_id\", LongType(), True),\n",
    "    StructField(\"notifications\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"order_timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "status_schema = StructType([\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"status_timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Create status streaming DataFrame\n",
    "status_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(status_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"path\", \"/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/status/stream_json\") \\\n",
    "    .load()\n",
    "\n",
    "# Create orders streaming DataFrame\n",
    "orders_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(orders_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"path\", \"/Volumes/databricks_simulated_retail_customer_data/v01/retail-pipeline/orders/stream_json\") \\\n",
    "    .load()\n",
    "\n",
    "# Add event_time column to status stream\n",
    "status_events = status_stream \\\n",
    "    .withColumn(\"event_time\", from_unixtime(col(\"status_timestamp\")).cast(\"timestamp\"))\n",
    "\n",
    "# Verify streams are set up correctly\n",
    "print(f\"orders_stream is streaming: {orders_stream.isStreaming}\")\n",
    "print(f\"status_stream is streaming: {status_stream.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dfefa53-b4e1-4559-932d-20f5e423004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stateful Operations\n",
    "\n",
    "Let's explore stateful operations that maintain state across micro-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfb67481-b1db-4b3a-b31d-d0260b903aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--<FILL IN>\n",
    "--# 1. Create a stateful aggregation that counts the number of orders by `order_status`\n",
    "--# 2. Create another stateful aggregation that counts orders by `customer_id`\n",
    "--# 3. Start streaming queries for both aggregations with `complete` output mode, writing to memory tables called \"status_counts\" and \"customer_counts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278a4be8-3821-4c64-b93c-4033e9525fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now you can query these tables to see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef453a78-3ba2-4246-b02c-7d23ec5e5a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Query the in-memory table to see status counts\n",
    "SELECT * FROM status_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7763a34e-b11b-43f5-a961-c1db12c25a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Query the in-memory table to see customer counts\n",
    "select * from customer_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf8955c3-c414-4f9c-96e7-7a194edefaa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sliding Window Operations\n",
    "In this section, you'll implement sliding window aggregations on the streaming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9ec1d3-24b9-40cf-809c-b6ca516e0c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--<FILL IN>\n",
    "--# 1. Create a sliding window aggregation on the status stream that:\n",
    "--#   - Groups by `order_status`\n",
    "--#   - Uses a window duration of 3 minutes\n",
    "--#   - Uses a sliding interval of 1 minute\n",
    "--#   - Counts the number of events in each window\n",
    "--# 2. Start a streaming query with this aggregation, using the `complete` output mode, writing to a memory table called \"sliding_windows\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb79d81-9db7-4902-ba25-b3b5ffafcd0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can query the sliding window results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa00f31-37ce-41f0-bfa4-af3f2b4dbc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Query the sliding window results\n",
    "SELECT \n",
    "  window.start as window_start,\n",
    "  window.end as window_end,\n",
    "  order_status,\n",
    "  count\n",
    "FROM sliding_windows\n",
    "ORDER BY window_start, order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1eaaf58-982f-4e37-8ed4-e79074bad044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Late Data Handling with Watermarks\n",
    "Now, let's explore how to handle late-arriving data using watermarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "775ebfaf-be4f-424c-9e53-4f73e1c1fc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--<FILL IN>\n",
    "--# 1. Modify your sliding window implementation to include a watermark of 5 minutes\n",
    "--# 2. Write the results to a memory table called \"windowed_with_watermark\"\n",
    "--# 3. Create another query that demonstrates a streaming join between orders and status with watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42341bcf-3593-4c0f-9f88-c303f802fb17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0bd32c8-234c-43ef-af52-0ddb781b0863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--# Query the windowed data with watermark\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10947666-9ca9-4af8-863f-fff825152dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--# TODO Query the joined data with watermarks\n",
    "<FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e7e892-6c20-4db4-b192-2be74eb73341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to stop the active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884649cb-ee59-4b00-bb25-2cc134e4cb64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6622290269133739,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stream Processing and Analysis with Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
